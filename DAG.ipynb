{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3PzXl5G9SEY/pcpn11qFw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paridhika/DDL/blob/main/DAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qtoqm5wxex6U"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "class DAGNode:\n",
        "    def __init__(self, task_name, job_name, start_time, end_time, status,cpu_avg,mem_avg,instance_num ):\n",
        "        self.task_name = task_name\n",
        "        self.job_name = job_name\n",
        "        self.start_time = start_time\n",
        "        self.end_time = end_time\n",
        "        self.status = status\n",
        "        self.dependencies = []\n",
        "        self.children = []  # New attribute to store directly connected children\n",
        "        self.cpu_avg = cpu_avg\n",
        "        self.mem_avg = mem_avg\n",
        "        self.instance_num  = instance_num\n",
        "        self.indegree = 0\n",
        "        self.outdegree = 0\n",
        "\n",
        "    def add_dependency(self, dependency_list):\n",
        "        self.dependencies = dependency_list\n",
        "\n",
        "    def add_child(self, child_node):\n",
        "        self.children.append(child_node)\n",
        "\n",
        "    def increment_indegree(self):\n",
        "        self.indegree += 1\n",
        "\n",
        "    def increment_outdegree(self):\n",
        "        self.outdegree += 1\n",
        "\n",
        "    def cumulative_lower_duration(self, total_duration):\n",
        "        self.cumulative_lower_duration = total_duration\n",
        "\n",
        "    def set_cumulative_upper_duration(self, cumulative_duration):\n",
        "        self.cumulative_upper_duration = cumulative_duration\n",
        "\n",
        "class DAGJob:\n",
        "    def __init__(self, job_name):\n",
        "        self.job_name = job_name\n",
        "        self.nodes = []\n",
        "        self.dependency_matrix = None  # Will be populated during processing\n",
        "        self.updated_weight_matrix =  None\n",
        "        self.job_start_time = 0\n",
        "        self.start_task = 1\n",
        "\n",
        "    def add_node(self, node):\n",
        "        self.nodes.append(node)\n",
        "\n",
        "\n",
        "\n",
        "    def generate_dependency_matrix(self):\n",
        "      num_nodes = len(self.nodes)\n",
        "      self.dependency_matrix = [[0] * num_nodes for _ in range(num_nodes)]\n",
        "      self.updated_weight_matrix = [[0.0] * num_nodes for _ in range(num_nodes)]\n",
        "\n",
        "      for node in self.nodes:\n",
        "          if node.dependencies != None:\n",
        "              for dependency_task in node.dependencies:\n",
        "                  # print(dependency_task)\n",
        "                  if len(self.dependency_matrix) < int(dependency_task):\n",
        "                      print(len(self.dependency_matrix))\n",
        "                  if len(self.dependency_matrix) < int(node.task_name):\n",
        "                      print(node.task_name)\n",
        "                  self.dependency_matrix[int(dependency_task)-1][int(node.task_name)-1] = 1\n",
        "\n",
        "    def display_nodes(self):\n",
        "        for node in self.nodes:\n",
        "            print(f\"Task: {node.task_name}, Job: {node.job_name}, Start Time: {node.start_time}, End Time: {node.end_time}, Status: {node.status}\")\n",
        "\n",
        "    def display_dependency_matrix(self):\n",
        "        for row in self.dependency_matrix:\n",
        "            print(row)\n",
        "\n",
        "    def has_all_nodes_connected(self):\n",
        "        if self.dependency_matrix == None:\n",
        "            return False\n",
        "        for i in range(len(self.dependency_matrix)):\n",
        "            if sum(self.dependency_matrix[i]) == 0 and sum(self.dependency_matrix[j][i] for j in range(len(self.dependency_matrix))) == 0:\n",
        "                # If a node has no incoming or outgoing edges, return False\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def update_start_times_based_on_dependencies(self):\n",
        "        # Create a dictionary to map task names to nodes\n",
        "        task_name_to_node = {node.task_name: node for node in self.nodes}\n",
        "\n",
        "        for node in self.nodes:\n",
        "            # Create a list of tasks with matching task names\n",
        "            matching_tasks = [task for task in self.nodes if int(task.task_name) in map(int, node.dependencies)]\n",
        "            # print(len(matching_tasks))\n",
        "            if matching_tasks:\n",
        "                max_end_time = max(int(task.end_time) for task in matching_tasks if task.end_time.isdigit())\n",
        "                # print(max_end_time)\n",
        "                # Check if node's start_time is a valid float\n",
        "                if node.start_time.isdigit():\n",
        "                    node_start_time = int(node.start_time)\n",
        "\n",
        "                    # Update start_time only if it's less than the max_end_time\n",
        "                    if node_start_time < max_end_time:\n",
        "                        node.start_time = str(max_end_time)\n",
        "\n",
        "    def generate_directly_connected_children(self):\n",
        "        for node in self.nodes:\n",
        "            for dependency_task in node.dependencies:\n",
        "                parent_node = next((n for n in self.nodes if int(n.task_name) == int(dependency_task)), None)\n",
        "                if parent_node:\n",
        "                    parent_node.add_child(node)\n",
        "\n",
        "\n",
        "    def generate_updated_weight_matrix(self):\n",
        "        num_nodes = len(self.nodes)\n",
        "        self.updated_weight_matrix = [[0.0] * num_nodes for _ in range(num_nodes)]\n",
        "\n",
        "        for node in self.nodes:\n",
        "            for dependency_task in node.dependencies:\n",
        "                parent_task = next((parent for parent in self.nodes if int(parent.task_name) == int(dependency_task)), None)\n",
        "                if parent_task:\n",
        "                    parent_task_index = int(parent_task.task_name) - 1\n",
        "                    node_index = int(node.task_name) - 1\n",
        "                    self.updated_weight_matrix[parent_task_index][node_index] = int(node.start_time) - int(parent_task.end_time)\n",
        "\n",
        "    def calculate_indegree_outdegree(self):\n",
        "        for node in self.nodes:\n",
        "            for dependency_task in node.dependencies:\n",
        "                parent_task = next((parent for parent in self.nodes if int(parent.task_name) == int(dependency_task)), None)\n",
        "                if parent_task:\n",
        "                    parent_task.increment_outdegree()\n",
        "                    node.increment_indegree()\n",
        "\n",
        "    def compute_lower_sum(self):\n",
        "        visited = set()\n",
        "\n",
        "        def dfs_recursive_lower(current_node):\n",
        "            nonlocal visited\n",
        "\n",
        "            visited.add(current_node)\n",
        "\n",
        "            lower_sum = 0\n",
        "\n",
        "            for child_task in current_node.children:\n",
        "                # child_node = next((n for n in sorted_nodes_indegree if int(n.task_name) == int(child_task)), None)\n",
        "                child_node = child_task\n",
        "                if child_node and child_node not in visited:\n",
        "                    lower_sum += dfs_recursive_lower(child_node) + int(child_node.duration)\n",
        "                elif child_node and child_node in visited:\n",
        "                    lower_sum += child_node.cumulative_lower_duration + int(child_node.duration)\n",
        "            current_node.cumulative_lower_duration = lower_sum\n",
        "            return lower_sum\n",
        "        # Sort nodes based on indegree in decreasing order and then taskname in decreasing order\n",
        "        sorted_nodes_indegree = sorted(self.nodes, key=lambda node: node.indegree, reverse=True)\n",
        "\n",
        "        # Start DFS from each node with in-degree 0\n",
        "        for node in sorted_nodes_indegree:\n",
        "            if node not in visited:\n",
        "                dfs_recursive_lower(node)\n",
        "\n",
        "\n",
        "    def compute_upper_sum(self):\n",
        "        visited = set()\n",
        "\n",
        "        def dfs_recursive_upper(current_node):\n",
        "            nonlocal visited\n",
        "\n",
        "            visited.add(current_node)\n",
        "\n",
        "            upper_sum = 0\n",
        "\n",
        "            for parent_task in current_node.dependencies:\n",
        "                parent_node = next((n for n in sorted_nodes_indegree if int(n.task_name) == int(parent_task)), None)\n",
        "                if parent_node and parent_node not in visited:\n",
        "                    upper_sum += dfs_recursive_upper(parent_node) + int(parent_node.duration)\n",
        "                elif parent_node and parent_node in visited:\n",
        "                    upper_sum += parent_node.cumulative_upper_duration  + int(parent_node.duration)\n",
        "            current_node.cumulative_upper_duration = upper_sum\n",
        "\n",
        "            return upper_sum\n",
        "\n",
        "        # Sort nodes based on outdegree\n",
        "        sorted_nodes_indegree = sorted(self.nodes, key=lambda node: node.indegree, reverse=True)\n",
        "\n",
        "        # Start DFS from each node with out-degree 0\n",
        "        for node in sorted_nodes_indegree:\n",
        "            if node not in visited:\n",
        "                dfs_recursive_upper(node)\n",
        "\n",
        "def create_dags_from_csv(file_path):\n",
        "    dag_jobs = {}\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8-sig') as csvfile:\n",
        "\n",
        "        reader = csv.DictReader(csvfile)\n",
        "\n",
        "        for row in reader:\n",
        "            # print(row)\n",
        "            job_name = row['job_name']\n",
        "            task_name = row['task_name']\n",
        "            if task_name.startswith(\"task_\"):\n",
        "                continue\n",
        "            start_time = row['start_time']\n",
        "            end_time = row['end_time']\n",
        "            status = row['status']\n",
        "            cpu_avg=row['plan_cpu']\n",
        "            mem_avg=row['plan_mem']\n",
        "            instance_num = row['instance_num']\n",
        "\n",
        "            # Extract the first number using regular expression\n",
        "            match = re.search(r'\\d+', task_name)\n",
        "\n",
        "            if match:\n",
        "                first_number = int(match.group())\n",
        "\n",
        "            node = DAGNode(first_number, job_name, start_time, end_time, status,cpu_avg,mem_avg,instance_num )\n",
        "\n",
        "            if job_name not in dag_jobs:\n",
        "                dag_jobs[job_name] = DAGJob(job_name)\n",
        "                dag_jobs[job_name].job_start_time = start_time\n",
        "                dag_jobs[job_name].start_task = first_number\n",
        "            existing_node = next((n for n in dag_jobs[job_name].nodes if n.task_name == node.task_name), None)\n",
        "            if int(start_time) < int(dag_jobs[job_name].job_start_time):\n",
        "                dag_jobs[job_name].job_start_time = start_time\n",
        "            if first_number < int(dag_jobs[job_name].start_task):\n",
        "                dag_jobs[job_name].start_task = first_number\n",
        "            if existing_node:\n",
        "                print(\"existing\")\n",
        "                # Merge by updating start_time and end_time\n",
        "                existing_node.start_time = min(existing_node.start_time, node.start_time)\n",
        "                existing_node.end_time = max(existing_node.end_time, node.end_time)\n",
        "            else:\n",
        "                dag_jobs[job_name].add_node(node)\n",
        "\n",
        "            # Extract dependencies from task_name\n",
        "            dependencies = [part for part in task_name.split('_') if part.isdigit()]\n",
        "            node.add_dependency(dependencies)\n",
        "\n",
        "    return dag_jobs\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "csv_file_path = 'tasks_3_to_20.csv'\n",
        "dag_jobs = create_dags_from_csv(csv_file_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove jobs which does not have dag structure\n",
        "jobs_to_remove = [job_name for job_name, dag_job in dag_jobs.items() if int(dag_job.start_task) != 1]\n",
        "\n",
        "for job_name in jobs_to_remove:\n",
        "    del dag_jobs[job_name]\n",
        "\n",
        "jobs_to_remove = []\n",
        "for job_name, dag_job in dag_jobs.items():\n",
        "    max_task_name = max(int(node.task_name) for node in dag_job.nodes)\n",
        "    if max_task_name > len(dag_job.nodes):\n",
        "        jobs_to_remove.append(job_name)\n",
        "\n",
        "for job_name in jobs_to_remove:\n",
        "    del dag_jobs[job_name]"
      ],
      "metadata": {
        "id": "ipuuHfODv816"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for job_name, dag_job in dag_jobs.items():\n",
        "    dag_job.generate_directly_connected_children()\n",
        "    dag_job.generate_dependency_matrix()\n",
        "    dag_job.update_start_times_based_on_dependencies()\n",
        "    dag_job.generate_updated_weight_matrix()\n",
        "    dag_job.calculate_indegree_outdegree()"
      ],
      "metadata": {
        "id": "zx-1RpWh111Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for job_name, dag_job in dag_jobs.items():\n",
        "    for node in dag_job.nodes:\n",
        "        node.duration = str(int(node.end_time) - int(node.start_time))\n",
        "        node.start_time = str(int(node.start_time) - int(dag_job.job_start_time))\n",
        "        node.end_time = str(int(node.end_time) - int(dag_job.job_start_time))"
      ],
      "metadata": {
        "id": "aWKMUwpPi4Kt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "connected_jobs = {}\n",
        "disconnected_jobs = {}\n",
        "\n",
        "for job_name, dag_job in dag_jobs.items():\n",
        "    if dag_job.has_all_nodes_connected():\n",
        "        connected_jobs[job_name] = dag_job\n",
        "    else:\n",
        "        disconnected_jobs[job_name] = dag_job\n"
      ],
      "metadata": {
        "id": "3KethMLqclT2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for job_name, dag_job in connected_jobs.items():\n",
        "    dag_job.compute_lower_sum()\n",
        "    dag_job.compute_upper_sum()\n",
        ""
      ],
      "metadata": {
        "id": "VKfIv1tzcW16"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(connected_jobs))\n",
        "import os\n",
        "# Ensure the output directory exists\n",
        "dot_output_directory = \"./connected_dot\"\n",
        "os.makedirs(dot_output_directory, exist_ok=True)\n",
        "\n",
        "for key,values in connected_jobs.items():\n",
        "    name = []\n",
        "\n",
        "    for node in values.nodes:\n",
        "        name.append(node.task_name)\n",
        "    dot_content = matrix_to_dot(values.dependency_matrix,name,values.updated_weight_matrix,values.nodes)\n",
        "    # Save DOT file\n",
        "    dot_file_path = os.path.join(dot_output_directory, key + \".dot\")\n",
        "    with open(dot_file_path, \"w\") as dot_file:\n",
        "        dot_file.write(dot_content)"
      ],
      "metadata": {
        "id": "okPms0R16Nil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from graphviz import Source\n",
        "\n",
        "# Directory containing DOT files\n",
        "dot_directory = \"./connected_dot\"\n",
        "\n",
        "# Output directory for PNG images\n",
        "png_output_directory = \"./connected_3-20\"\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(png_output_directory, exist_ok=True)\n",
        "\n",
        "# Iterate over DOT files in the directory\n",
        "for dot_filename in os.listdir(dot_directory):\n",
        "    if dot_filename.endswith(\".dot\"):\n",
        "        # Construct the full paths for DOT and PNG files\n",
        "        dot_file_path = os.path.join(dot_directory, dot_filename)\n",
        "        png_output_path = os.path.join(png_output_directory, os.path.splitext(dot_filename)[0])\n",
        "\n",
        "        # Read the DOT file\n",
        "        source = Source.from_file(dot_file_path, format=\"png\")\n",
        "\n",
        "        # Save the PNG image\n",
        "        source.render(png_output_path, format=\"png\", cleanup=True)\n",
        "\n",
        "        # print(f\"PNG image created: {png_output_path}\")\n",
        "        os.remove(dot_file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "fOd09otWb-aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install graphviz\n",
        "import os\n",
        "from graphviz import Digraph\n",
        "\n",
        "def create_dot_file(dag, output_file_path):\n",
        "    dot = Digraph(comment='Directed Acyclic Graph', format='png')\n",
        "\n",
        "    num_nodes = len(dag.nodes)\n",
        "    # Add nodes\n",
        "    for node in dag.nodes:\n",
        "        name = str(node.task_name) if node is not None else \"\"\n",
        "        start_time = \"\" if node.start_time is None else f'\\nStart Time: {node.start_time}'\n",
        "        end_time = \"\" if node.end_time is None else f'\\nEnd Time: {node.end_time}'\n",
        "        duration = \"\" if node.duration is None else f'\\nDuration: {node.duration}'\n",
        "        upper_sum = \"\" if node.cumulative_upper_duration is None else f'\\nUpper: {node.cumulative_upper_duration}'\n",
        "        lower_sum = \"\" if node.cumulative_lower_duration is None else f'\\nLower: {node.cumulative_lower_duration}'\n",
        "        instance_num = \"\" if node.instance_num is None else f'\\n#instances: {node.instance_num}'\n",
        "        cpu = \"\" if node.cpu_avg is None else f'\\nCPU: {node.cpu_avg}'\n",
        "        dot.node(str(name), label=f'{start_time}{duration}{upper_sum}{lower_sum}{cpu}{instance_num}')\n",
        "\n",
        "    # Add edges\n",
        "    for row in range(num_nodes):\n",
        "        for column in range(num_nodes):\n",
        "            if dag.dependency_matrix[row][column] == 0:\n",
        "                continue\n",
        "            weight = \"\" if dag.updated_weight_matrix is None else f'{dag.updated_weight_matrix[row][column]}'\n",
        "            dot.edge(str(row + 1), str(column + 1), label=f'{weight}')\n",
        "\n",
        "    dot.render(output_file_path, view=True)\n",
        "\n",
        "# Ensure the output directory exists\n",
        "dot_output_directory = \"./connected_dot/\"\n",
        "os.makedirs(dot_output_directory, exist_ok=True)\n",
        "\n",
        "for key, dag in connected_jobs.items():\n",
        "    create_dot_file(dag, os.path.join(dot_output_directory, key + \".gv\"))\n",
        "\n",
        "def matrix_to_dot(matrix, node_labels=None, edge_weights=None,task_node=None):\n",
        "    \"\"\"\n",
        "    Convert an adjacency matrix with weights to Graphviz DOT format.\n",
        "\n",
        "    Parameters:\n",
        "    - matrix: List of lists representing the adjacency matrix.\n",
        "    - node_labels: Optional list of node labels.\n",
        "    - edge_weights: Optional list of lists representing edge weights.\n",
        "\n",
        "    Returns:\n",
        "    - dot_content: String containing the DOT representation.\n",
        "    \"\"\"\n",
        "    num_nodes = len(matrix)\n",
        "\n",
        "    # Create DOT header\n",
        "    dot_content = \"digraph MyDAG {\\n\"\n",
        "\n",
        "    # Add nodes\n",
        "    for node in range(num_nodes):\n",
        "        label = str(node + 1) if node_labels is None else str(node_labels[node])\n",
        "        start_time = \"\" if task_node[node].start_time is None else f'\\nStart Time: {task_node[node].start_time}'\n",
        "        end_time = \"\" if task_node[node].end_time is None else f'\\nEnd Time: {task_node[node].end_time}'\n",
        "        duration = \"\" if task_node[node].duration is None else f'\\nDuration: {task_node[node].duration}'\n",
        "        upper_sum = \"\" if task_node[node].cumulative_upper_duration is None else f'\\nUpper: {task_node[node].cumulative_upper_duration}'\n",
        "        lower_sum = \"\" if task_node[node].cumulative_lower_duration is None else f'\\nLower: {task_node[node].cumulative_lower_duration}'\n",
        "        instance_num = \"\" if task_node[node].instance_num is None else f'\\n#instances: {task_node[node].instance_num}'\n",
        "        cpu = \"\" if task_node[node].cpu_avg is None else f'\\nCPU: {task_node[node].cpu_avg}'\n",
        "        dot_content += f'  {str(node_labels[node])} [label=\"{label}{start_time}{duration}{upper_sum}{lower_sum}{cpu}\"];\\n'\n",
        "\n",
        "    # for node in range(num_nodes):\n",
        "    #     label = str(node + 1) if node_labels is None else str(node_labels[node])\n",
        "    #     dot_content += f'  {node + 1} [label=\"{label}\"];\\n'\n",
        "\n",
        "    # Add edges with weights\n",
        "    for row in range(num_nodes):\n",
        "        for column in range(num_nodes):\n",
        "          if matrix[row][column] != 1:\n",
        "            continue\n",
        "          weight = \"\" if edge_weights is None else f' [label=\"{edge_weights[row][column]}\"]'\n",
        "          dot_content += f'  {row + 1} -> {column + 1}{weight};\\n'\n",
        "\n",
        "    # Close DOT file\n",
        "    dot_content += \"}\"\n",
        "\n",
        "    return dot_content\n"
      ],
      "metadata": {
        "id": "ps2oq3cyAffG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(disconnected_jobs))\n",
        "\n",
        "# Ensure the output directory exists\n",
        "dot_output_directory = \"./disconnected_dot\"\n",
        "os.makedirs(dot_output_directory, exist_ok=True)\n",
        "for key,values in disconnected_jobs.items():\n",
        "    name = []\n",
        "\n",
        "    for node in values.nodes:\n",
        "        name.append(node.task_name)\n",
        "    dot_content = matrix_to_dot(values.dependency_matrix,name,values.updated_weight_matrix,values.nodes)\n",
        "    # Save DOT file\n",
        "    dot_file_path = os.path.join(dot_output_directory, key + \".dot\")\n",
        "    with open(dot_file_path, \"w\") as dot_file:\n",
        "        dot_file.write(dot_content)"
      ],
      "metadata": {
        "id": "0BBl-vg3jDDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_directory = \"./disconnected_dot\"\n",
        "\n",
        "# Output directory for PNG images\n",
        "png_output_directory = \"./disconnected_3-20\"\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(png_output_directory, exist_ok=True)\n",
        "\n",
        "# Iterate over DOT files in the directory\n",
        "for dot_filename in os.listdir(dot_directory):\n",
        "    if dot_filename.endswith(\".dot\"):\n",
        "        # Construct the full paths for DOT and PNG files\n",
        "        dot_file_path = os.path.join(dot_directory, dot_filename)\n",
        "        png_output_path = os.path.join(png_output_directory, os.path.splitext(dot_filename)[0])\n",
        "\n",
        "        # Read the DOT file\n",
        "        source = Source.from_file(dot_file_path, format=\"png\")\n",
        "\n",
        "        # Save the PNG image\n",
        "        source.render(png_output_path, format=\"png\", cleanup=True)\n",
        "\n",
        "        # print(f\"PNG image created: {png_output_path}\")\n",
        "        os.remove(dot_file_path)"
      ],
      "metadata": {
        "id": "mFCoP5GOjTnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def zip_folder(folder_path, zip_path):\n",
        "    shutil.make_archive(zip_path, 'zip', folder_path)\n",
        "    shutil.rmtree(folder_path)\n",
        "\n",
        "# Replace 'path_to_folder' and 'path_to_zipfile' with your actual folder and desired zip file paths\n",
        "folder_path = '/content/connected_3-20'\n",
        "zip_path = '/content/output_connected_3-20'\n",
        "\n",
        "zip_folder(folder_path, zip_path)\n",
        "\n",
        "folder_path = '/content/disconnected_3-20'\n",
        "zip_path = '/content/output_disconnected_3-20'\n",
        "\n",
        "zip_folder(folder_path, zip_path)"
      ],
      "metadata": {
        "id": "5V3Ne8_9yi5E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}