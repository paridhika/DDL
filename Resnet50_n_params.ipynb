{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paridhika/DDL/blob/main/Resnet50_n_params.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# According to PyTorch"
      ],
      "metadata": {
        "id": "uN3qotb__Mq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "from torchvision.models import resnet50\n",
        "import torch\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "ETJGbo7g_IqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(resnet50(), (3, 224, 224))"
      ],
      "metadata": {
        "id": "E0MxyiMD_WVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# According to TF"
      ],
      "metadata": {
        "id": "HZGYh8Sd_cG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "_DfmwWFc_v6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.applications.resnet.ResNet50(weights=None)\n",
        "# model.summary();"
      ],
      "metadata": {
        "id": "FIvEneg__xM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get layers with tensors"
      ],
      "metadata": {
        "id": "Es2EGNx0Y4Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.math_ops import sub\n",
        "# import tensorflow as tf\n",
        "# model = tf.keras.applications.resnet.ResNet50(weights='imagenet');\n",
        "# model.summary();\n",
        "import csv  \n",
        "layer_count = 0\n",
        "my_map = {}\n",
        "for i, layer in enumerate(model.layers):\n",
        "    if \"add\" not in layer.name:\n",
        "        layer_count += 1\n",
        "        # print(\"Layer {}: {} ({} parameters)\".format(layer_count, layer.name, layer.count_params()))\n",
        "        my_map[layer.name] = layer.count_params()\n",
        "  \n",
        "print(len(my_map))\n",
        "\n",
        "map_list = list(my_map.items())\n",
        "map_list.reverse()\n",
        "with open(\"tensor_layers.csv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(map_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH8P5T57Y_Eb",
        "outputId": "c91ec407-7abe-46a3-964a-f24eb96921ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get layers with parameters"
      ],
      "metadata": {
        "id": "RO7kxoucZxRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.math_ops import sub\n",
        "# import tensorflow as tf\n",
        "# model = tf.keras.applications.resnet.ResNet50(weights='imagenet');\n",
        "# model.summary();\n",
        "import csv  \n",
        "layer_count = 0\n",
        "my_map = {}\n",
        "for i, layer in enumerate(model.layers):\n",
        "    if len(layer.weights) > 0:\n",
        "      if \"add\" not in layer.name:\n",
        "        layer_count += 1\n",
        "        # print(\"Layer {}: {} ({} parameters)\".format(layer_count, layer.name, layer.count_params()))\n",
        "        my_map[layer.name] = layer.count_params()\n",
        "  \n",
        "print(len(my_map))\n",
        "\n",
        "map_list = list(my_map.items())\n",
        "map_list.reverse()\n",
        "with open(\"parameters_layers.csv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(map_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaKp_Oo3Z0oi",
        "outputId": "bbe350b7-6d58-4549-810a-105b9f8bc16a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute burst size"
      ],
      "metadata": {
        "id": "0OokMaVYPjXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.math_ops import sub\n",
        "# import tensorflow as tf\n",
        "# model = tf.keras.applications.resnet.ResNet50(weights='imagenet');\n",
        "# model.summary();\n",
        "import csv  \n",
        "layer_count = 0\n",
        "substring = \"\"\n",
        "my_map = {}\n",
        "\n",
        "\n",
        "for i, layer in enumerate(model.layers):\n",
        "    if len(layer.weights) > 0:\n",
        "      if \"_\" in layer.name:\n",
        "        layer_count += 1\n",
        "        # print(\"Layer {}: {} ({} parameters)\".format(layer_count, layer.name, layer.count_params()))\n",
        "        substring = layer.name[:layer.name.rindex(\"_\")]\n",
        "        if substring in my_map:\n",
        "          my_map[substring] = my_map[substring] + layer.count_params()\n",
        "        else:\n",
        "          my_map[substring] = layer.count_params()\n",
        "      else:\n",
        "        my_map[layer.name] = layer.count_params()\n",
        "  \n",
        "print(len(my_map))\n",
        "my_map[\"Layer Name\"] = \"Theoritical\"\n",
        "map_list = list(my_map.items())\n",
        "map_list.reverse()\n",
        "with open(\"my_map.csv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(map_list)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtXakP2APrc2",
        "outputId": "ae9803a6-8164-4c91-e466-a98a6ed08ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get table of comparison between theoritical and experimental burst sizes**"
      ],
      "metadata": {
        "id": "jjI6pNG5g_Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "def generate_latex_table(filename):\n",
        "    with open(filename, newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        header = next(reader)\n",
        "        table = []\n",
        "        for row in reader:\n",
        "            table.append(row)\n",
        "        \n",
        "    table_string = \"\\\\begin{tabular}{|\" + \"c|\"*len(header) + \"}\\n\\\\hline\\n\"\n",
        "    table_string += \" & \".join(header) + \"\\\\\\\\\\n\\\\hline\\n\"\n",
        "    for row in table:\n",
        "        table_string += \" & \".join(row) + \"\\\\\\\\\\n\\\\hline\\n\"\n",
        "    table_string += \"\\\\end{tabular}\"\n",
        "    \n",
        "    return table_string\n",
        "\n",
        "numbers = [2049839.5,1048383.5,2359135.5,1048447.5,1048367.5,2359167.5,1048367.5,1048367.5,2096959.5,2359311.5,524079.5,262015.5,589647.5,261983.5,261951.5,589695.5,261951.5,262015.5,589615.5,261935.5,261935.5,589663.5,262015.5,262527.5,589615.5,261919.5,261935.5,524143.5,589599.5,130943.5,65359.5,147231.5,65375.5,65551.5,147247.5,65343.5,65391.5,147231.5,65327.5,65551.5,130879.5,147231.5,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "rounded_values = [int(round(number)) for number in numbers]\n",
        "\n",
        "# Load the existing CSV file into a pandas dataframe\n",
        "df = pd.read_csv(\"my_map.csv\")\n",
        "\n",
        "# Create a new column in the dataframe with the rounded integer values\n",
        "df[\"Experimental\"] = rounded_values\n",
        "\n",
        "# Save the updated dataframe to the existing CSV file\n",
        "df.to_csv(\"my_map.csv\", index=False)\n",
        "\n",
        "filename = 'my_map.csv'\n",
        "index_col = [i for i in range(1, 55)] #index column\n",
        "index_col = index_col[::-1]\n",
        "with open(filename, 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    header = next(reader) #get header\n",
        "    header.insert(0, 'Burst Count') #insert \"Index\" header in the first position\n",
        "    data = [row for row in reader] #get data\n",
        "\n",
        "#create new data with index column\n",
        "new_data = []\n",
        "for i, row in enumerate(data):\n",
        "    row.insert(0, index_col[i])\n",
        "    new_data.append(row)\n",
        "\n",
        "#write data to a new file\n",
        "with open('new_file.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(new_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "FTUIVtaDeERw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Open the CSV file for reading\n",
        "with open('new_file.csv', 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    data = [row for row in reader]\n",
        "\n",
        "# Round off all floating point values to integers\n",
        "for i, row in enumerate(data):\n",
        "    data[i] = [int(float(cell)) if '.' in cell else cell for cell in row]\n",
        "\n",
        "# Write the updated data back to the same CSV file\n",
        "with open('final.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerows(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "wQg4s5i5x9jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_string = generate_latex_table(\"new_file.csv\")\n",
        "with open('latex_bursts.tex', 'w') as f:\n",
        "        f.write(table_string)"
      ],
      "metadata": {
        "id": "yjUwWObHz0p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute number of parameters by formula**"
      ],
      "metadata": {
        "id": "nIX0x58yJ0Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Instantiate the ResNet50 model\n",
        "model =  tf.keras.applications.resnet.ResNet50(weights=None) \n",
        "\n",
        "# Define a dictionary to store the number of operations for each layer\n",
        "ops_dict = {}\n",
        "param_dict = {}\n",
        "# Define a function to compute the number of operations for a Conv2D layer\n",
        "def compute_ops_conv2d(layer):\n",
        "    kernel_size = layer.kernel_size[0]\n",
        "    in_channels = layer.input_shape[-1]\n",
        "    out_channels = layer.output_shape[-1]\n",
        "    ops = kernel_size**2 * in_channels * out_channels\n",
        "    return ops\n",
        "\n",
        "# Define a function to compute the number of operations for a MaxPooling2D layer\n",
        "def compute_ops_maxpool2d(layer):\n",
        "    pool_size = layer.pool_size[0]\n",
        "    in_channels = layer.input_shape[-1]\n",
        "    ops = pool_size**2 * in_channels\n",
        "    return ops\n",
        "\n",
        "# Define a function to compute the number of operations for a ReLU layer\n",
        "def compute_ops_relu(layer):\n",
        "    num_elements = tf.reduce_prod(layer.input_shape[1:])\n",
        "    ops = num_elements\n",
        "    return ops\n",
        "\n",
        "# Define a function to compute the number of operations for the output layer\n",
        "def compute_ops_output(layer):\n",
        "    num_input_channels = layer.input_shape[-1]\n",
        "    num_output_channels = layer.output_shape[-1]\n",
        "    fc_ops = (num_input_channels * num_output_channels) + num_output_channels\n",
        "    if type(layer.activation) == tf.keras.activations.softmax or type(layer.activation) ==  tf.keras.activations.sigmoid:\n",
        "        num_elements = tf.reduce_prod(layer.output_shape[1:])\n",
        "        act_ops = num_elements\n",
        "    else:\n",
        "        act_ops = 0\n",
        "    ops = fc_ops + act_ops\n",
        "    return ops\n",
        "\n",
        "# Compute the number of operations for each layer in the model\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "        ops = compute_ops_conv2d(layer)\n",
        "    elif isinstance(layer, tf.keras.layers.MaxPooling2D):\n",
        "        ops = compute_ops_maxpool2d(layer)\n",
        "    elif isinstance(layer, tf.keras.layers.Activation) and type(layer.activation) == tf.keras.activations.relu:\n",
        "        ops = compute_ops_relu(layer)\n",
        "    elif isinstance(layer, tf.keras.layers.Dense):\n",
        "        ops = compute_ops_output(layer)\n",
        "    elif isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "        num_channels = layer.output_shape[-1]\n",
        "        ops = 4 * num_channels\n",
        "    else:\n",
        "        ops = 0\n",
        "    ops_dict[layer.name] = ops\n",
        "    param_dict[layer.name] = layer.count_params()\n",
        "# Print the number of operations for each layer in the model\n",
        "for layer_name, ops in ops_dict.items():\n",
        "    print(layer_name, ops, param_dict[layer_name])\n"
      ],
      "metadata": {
        "id": "Q5IPj3YOEwAm",
        "outputId": "128b743b-2ca6-49d5-e6be-1586ca5761cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_3 0 0\n",
            "conv1_pad 0 0\n",
            "conv1_conv 9408 9472\n",
            "conv1_bn 256 256\n",
            "conv1_relu 0 0\n",
            "pool1_pad 0 0\n",
            "pool1_pool 576 0\n",
            "conv2_block1_1_conv 4096 4160\n",
            "conv2_block1_1_bn 256 256\n",
            "conv2_block1_1_relu 0 0\n",
            "conv2_block1_2_conv 36864 36928\n",
            "conv2_block1_2_bn 256 256\n",
            "conv2_block1_2_relu 0 0\n",
            "conv2_block1_0_conv 16384 16640\n",
            "conv2_block1_3_conv 16384 16640\n",
            "conv2_block1_0_bn 1024 1024\n",
            "conv2_block1_3_bn 1024 1024\n",
            "conv2_block1_add 0 0\n",
            "conv2_block1_out 0 0\n",
            "conv2_block2_1_conv 16384 16448\n",
            "conv2_block2_1_bn 256 256\n",
            "conv2_block2_1_relu 0 0\n",
            "conv2_block2_2_conv 36864 36928\n",
            "conv2_block2_2_bn 256 256\n",
            "conv2_block2_2_relu 0 0\n",
            "conv2_block2_3_conv 16384 16640\n",
            "conv2_block2_3_bn 1024 1024\n",
            "conv2_block2_add 0 0\n",
            "conv2_block2_out 0 0\n",
            "conv2_block3_1_conv 16384 16448\n",
            "conv2_block3_1_bn 256 256\n",
            "conv2_block3_1_relu 0 0\n",
            "conv2_block3_2_conv 36864 36928\n",
            "conv2_block3_2_bn 256 256\n",
            "conv2_block3_2_relu 0 0\n",
            "conv2_block3_3_conv 16384 16640\n",
            "conv2_block3_3_bn 1024 1024\n",
            "conv2_block3_add 0 0\n",
            "conv2_block3_out 0 0\n",
            "conv3_block1_1_conv 32768 32896\n",
            "conv3_block1_1_bn 512 512\n",
            "conv3_block1_1_relu 0 0\n",
            "conv3_block1_2_conv 147456 147584\n",
            "conv3_block1_2_bn 512 512\n",
            "conv3_block1_2_relu 0 0\n",
            "conv3_block1_0_conv 131072 131584\n",
            "conv3_block1_3_conv 65536 66048\n",
            "conv3_block1_0_bn 2048 2048\n",
            "conv3_block1_3_bn 2048 2048\n",
            "conv3_block1_add 0 0\n",
            "conv3_block1_out 0 0\n",
            "conv3_block2_1_conv 65536 65664\n",
            "conv3_block2_1_bn 512 512\n",
            "conv3_block2_1_relu 0 0\n",
            "conv3_block2_2_conv 147456 147584\n",
            "conv3_block2_2_bn 512 512\n",
            "conv3_block2_2_relu 0 0\n",
            "conv3_block2_3_conv 65536 66048\n",
            "conv3_block2_3_bn 2048 2048\n",
            "conv3_block2_add 0 0\n",
            "conv3_block2_out 0 0\n",
            "conv3_block3_1_conv 65536 65664\n",
            "conv3_block3_1_bn 512 512\n",
            "conv3_block3_1_relu 0 0\n",
            "conv3_block3_2_conv 147456 147584\n",
            "conv3_block3_2_bn 512 512\n",
            "conv3_block3_2_relu 0 0\n",
            "conv3_block3_3_conv 65536 66048\n",
            "conv3_block3_3_bn 2048 2048\n",
            "conv3_block3_add 0 0\n",
            "conv3_block3_out 0 0\n",
            "conv3_block4_1_conv 65536 65664\n",
            "conv3_block4_1_bn 512 512\n",
            "conv3_block4_1_relu 0 0\n",
            "conv3_block4_2_conv 147456 147584\n",
            "conv3_block4_2_bn 512 512\n",
            "conv3_block4_2_relu 0 0\n",
            "conv3_block4_3_conv 65536 66048\n",
            "conv3_block4_3_bn 2048 2048\n",
            "conv3_block4_add 0 0\n",
            "conv3_block4_out 0 0\n",
            "conv4_block1_1_conv 131072 131328\n",
            "conv4_block1_1_bn 1024 1024\n",
            "conv4_block1_1_relu 0 0\n",
            "conv4_block1_2_conv 589824 590080\n",
            "conv4_block1_2_bn 1024 1024\n",
            "conv4_block1_2_relu 0 0\n",
            "conv4_block1_0_conv 524288 525312\n",
            "conv4_block1_3_conv 262144 263168\n",
            "conv4_block1_0_bn 4096 4096\n",
            "conv4_block1_3_bn 4096 4096\n",
            "conv4_block1_add 0 0\n",
            "conv4_block1_out 0 0\n",
            "conv4_block2_1_conv 262144 262400\n",
            "conv4_block2_1_bn 1024 1024\n",
            "conv4_block2_1_relu 0 0\n",
            "conv4_block2_2_conv 589824 590080\n",
            "conv4_block2_2_bn 1024 1024\n",
            "conv4_block2_2_relu 0 0\n",
            "conv4_block2_3_conv 262144 263168\n",
            "conv4_block2_3_bn 4096 4096\n",
            "conv4_block2_add 0 0\n",
            "conv4_block2_out 0 0\n",
            "conv4_block3_1_conv 262144 262400\n",
            "conv4_block3_1_bn 1024 1024\n",
            "conv4_block3_1_relu 0 0\n",
            "conv4_block3_2_conv 589824 590080\n",
            "conv4_block3_2_bn 1024 1024\n",
            "conv4_block3_2_relu 0 0\n",
            "conv4_block3_3_conv 262144 263168\n",
            "conv4_block3_3_bn 4096 4096\n",
            "conv4_block3_add 0 0\n",
            "conv4_block3_out 0 0\n",
            "conv4_block4_1_conv 262144 262400\n",
            "conv4_block4_1_bn 1024 1024\n",
            "conv4_block4_1_relu 0 0\n",
            "conv4_block4_2_conv 589824 590080\n",
            "conv4_block4_2_bn 1024 1024\n",
            "conv4_block4_2_relu 0 0\n",
            "conv4_block4_3_conv 262144 263168\n",
            "conv4_block4_3_bn 4096 4096\n",
            "conv4_block4_add 0 0\n",
            "conv4_block4_out 0 0\n",
            "conv4_block5_1_conv 262144 262400\n",
            "conv4_block5_1_bn 1024 1024\n",
            "conv4_block5_1_relu 0 0\n",
            "conv4_block5_2_conv 589824 590080\n",
            "conv4_block5_2_bn 1024 1024\n",
            "conv4_block5_2_relu 0 0\n",
            "conv4_block5_3_conv 262144 263168\n",
            "conv4_block5_3_bn 4096 4096\n",
            "conv4_block5_add 0 0\n",
            "conv4_block5_out 0 0\n",
            "conv4_block6_1_conv 262144 262400\n",
            "conv4_block6_1_bn 1024 1024\n",
            "conv4_block6_1_relu 0 0\n",
            "conv4_block6_2_conv 589824 590080\n",
            "conv4_block6_2_bn 1024 1024\n",
            "conv4_block6_2_relu 0 0\n",
            "conv4_block6_3_conv 262144 263168\n",
            "conv4_block6_3_bn 4096 4096\n",
            "conv4_block6_add 0 0\n",
            "conv4_block6_out 0 0\n",
            "conv5_block1_1_conv 524288 524800\n",
            "conv5_block1_1_bn 2048 2048\n",
            "conv5_block1_1_relu 0 0\n",
            "conv5_block1_2_conv 2359296 2359808\n",
            "conv5_block1_2_bn 2048 2048\n",
            "conv5_block1_2_relu 0 0\n",
            "conv5_block1_0_conv 2097152 2099200\n",
            "conv5_block1_3_conv 1048576 1050624\n",
            "conv5_block1_0_bn 8192 8192\n",
            "conv5_block1_3_bn 8192 8192\n",
            "conv5_block1_add 0 0\n",
            "conv5_block1_out 0 0\n",
            "conv5_block2_1_conv 1048576 1049088\n",
            "conv5_block2_1_bn 2048 2048\n",
            "conv5_block2_1_relu 0 0\n",
            "conv5_block2_2_conv 2359296 2359808\n",
            "conv5_block2_2_bn 2048 2048\n",
            "conv5_block2_2_relu 0 0\n",
            "conv5_block2_3_conv 1048576 1050624\n",
            "conv5_block2_3_bn 8192 8192\n",
            "conv5_block2_add 0 0\n",
            "conv5_block2_out 0 0\n",
            "conv5_block3_1_conv 1048576 1049088\n",
            "conv5_block3_1_bn 2048 2048\n",
            "conv5_block3_1_relu 0 0\n",
            "conv5_block3_2_conv 2359296 2359808\n",
            "conv5_block3_2_bn 2048 2048\n",
            "conv5_block3_2_relu 0 0\n",
            "conv5_block3_3_conv 1048576 1050624\n",
            "conv5_block3_3_bn 8192 8192\n",
            "conv5_block3_add 0 0\n",
            "conv5_block3_out 0 0\n",
            "avg_pool 0 0\n",
            "predictions 2049000 2049000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make plot for comparison**"
      ],
      "metadata": {
        "id": "a3Y8rY0a45k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the csv file into a pandas DataFrame\n",
        "df = pd.read_csv('my_map.csv')\n",
        "\n",
        "# Extract the two columns you need\n",
        "col1 = df['Theoritical']\n",
        "col2 = df['Experimental']\n",
        "\n",
        "# Plot the histogram\n",
        "plt.plot(df.iloc[:43, 1]/1000000, color='blue', label='Theoritical')\n",
        "plt.plot(df.iloc[:43, 2]/1000000, color='red', label='Experimental')\n",
        "\n",
        "# Add a legend and labels for the x and y axes\n",
        "plt.legend()\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Data (MB)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aaoQ80nJ4-Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute computation time"
      ],
      "metadata": {
        "id": "wNLSamFKnlos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Instantiate the ResNet50 model\n",
        "model =  tf.keras.applications.resnet.ResNet50(weights=None) \n",
        "\n",
        "# Print the input and output shape for each convolutional layer in the model\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "      print(layer.name, layer.input_shape, layer.output_shape,  layer.kernel_size, layer.filters)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JeLauYSmnr0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Instantiate the ResNet50 model\n",
        "model =  tf.keras.applications.resnet.ResNet50(weights=None) \n",
        "\n",
        "# Define a dictionary to store the number of operations for each layer\n",
        "ops_dict = {}\n",
        "\n",
        "# Define a function to compute the number of operations for a Conv2D layer\n",
        "def compute_ops_conv2d(layer):\n",
        "    kernel_size = layer.kernel_size[0]\n",
        "    in_channels = layer.input_shape[-1]\n",
        "    out_channels = layer.output_shape[-1]\n",
        "    input_height, input_width = layer.input_shape[1:3]\n",
        "    output_height, output_width = layer.output_shape[1:3]\n",
        "    ops = kernel_size**2 * in_channels * output_height * output_width * out_channels\n",
        "    return ops\n",
        "\n",
        "# Define a function to compute the number of operations for a MaxPooling2D layer\n",
        "def compute_ops_maxpool2d(layer):\n",
        "    pool_size = layer.pool_size[0]\n",
        "    in_channels = layer.input_shape[-1]\n",
        "    input_height, input_width = layer.input_shape[1:3]\n",
        "    output_height, output_width = layer.output_shape[1:3]\n",
        "    ops = pool_size**2 * in_channels * output_height * output_width\n",
        "    return ops\n",
        "\n",
        "# Define a function to compute the number of operations for a BatchNormalization layer\n",
        "def compute_ops_batchnorm(layer):\n",
        "    # Get input shape\n",
        "    input_shape = layer.input_shape[1:]\n",
        "    in_channels = input_shape[-1]\n",
        "\n",
        "    # Number of operations per element (2 for mean and variance)\n",
        "    num_ops = 2 * in_channels\n",
        "\n",
        "    return num_ops\n",
        "\n",
        "# Define a function to compute the number of operations for a ReLU layer\n",
        "def compute_ops_relu(layer):\n",
        "    # Get output shape\n",
        "    output_shape = layer.output_shape[1:]\n",
        "    num_elements = np.prod(output_shape)\n",
        "    input_shape = layer.input_shape\n",
        "    # Number of operations per element\n",
        "    num_ops = 1\n",
        "\n",
        "    # Total number of operations\n",
        "    total_ops = num_elements * num_ops\n",
        "\n",
        "    # Add operations for global average pooling\n",
        "    if input_shape[1] != output_shape[1] or input_shape[2] != output_shape[2]:\n",
        "        pool_size = (input_shape[1] - output_shape[1] + 1, input_shape[2] - output_shape[2] + 1)\n",
        "        num_pool_ops = np.prod(pool_size) * output_shape[-1]\n",
        "        total_ops += num_pool_ops\n",
        "\n",
        "    return total_ops\n",
        "\n",
        "# Define a function to compute the number of operations for the output layer\n",
        "def compute_ops_output(layer):\n",
        "    num_input_channels = layer.input_shape[-1]\n",
        "    num_output_channels = layer.output_shape[-1]\n",
        "    fc_ops = (num_input_channels * num_output_channels) + num_output_channels\n",
        "    if type(layer.activation) == tf.keras.activations.softmax or type(layer.activation) == tf.keras.activations.sigmoid:\n",
        "        num_elements = tf.reduce_prod(layer.output_shape[1:])\n",
        "        act_ops = num_elements\n",
        "    else:\n",
        "        act_ops = 0\n",
        "    ops = fc_ops + act_ops\n",
        "    return ops\n",
        "\n",
        "# Compute the number of operations for each layer in the model\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "        ops = compute_ops_conv2d(layer)\n",
        "    elif isinstance(layer, tf.keras.layers.MaxPooling2D):\n",
        "        ops = compute_ops_maxpool2d(layer)\n",
        "    elif isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "        ops = compute_ops_batchnorm(layer)\n",
        "    elif isinstance(layer, tf.keras.layers.Activation) and type(layer.activation) == tf.keras.activations.relu:\n",
        "        ops = compute_ops_relu(layer)\n",
        "    elif isinstance(layer, tf.keras.layers.Dense):\n",
        "        ops = compute_ops_output(layer)\n",
        "    else:\n",
        "        ops = 0\n",
        "    ops_dict[layer.name] = ops\n",
        "\n",
        "# Print the number of operations for each layer in the model\n",
        "sum = 0;\n",
        "batch_size = 32\n",
        "for layer_name, ops in ops_dict.items():\n",
        "    sum = sum + ops\n",
        "    print(layer_name, ops)\n",
        "print(sum * batch_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "1b1o-QJFKJZv",
        "outputId": "2f11e6ba-271c-4746-96b3-6718e37fcde8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_6 0\n",
            "conv1_pad 0\n",
            "conv1_conv 118013952\n",
            "conv1_bn 128\n",
            "conv1_relu 0\n",
            "pool1_pad 0\n",
            "pool1_pool 1806336\n",
            "conv2_block1_1_conv 12845056\n",
            "conv2_block1_1_bn 128\n",
            "conv2_block1_1_relu 0\n",
            "conv2_block1_2_conv 115605504\n",
            "conv2_block1_2_bn 128\n",
            "conv2_block1_2_relu 0\n",
            "conv2_block1_0_conv 51380224\n",
            "conv2_block1_3_conv 51380224\n",
            "conv2_block1_0_bn 512\n",
            "conv2_block1_3_bn 512\n",
            "conv2_block1_add 0\n",
            "conv2_block1_out 0\n",
            "conv2_block2_1_conv 51380224\n",
            "conv2_block2_1_bn 128\n",
            "conv2_block2_1_relu 0\n",
            "conv2_block2_2_conv 115605504\n",
            "conv2_block2_2_bn 128\n",
            "conv2_block2_2_relu 0\n",
            "conv2_block2_3_conv 51380224\n",
            "conv2_block2_3_bn 512\n",
            "conv2_block2_add 0\n",
            "conv2_block2_out 0\n",
            "conv2_block3_1_conv 51380224\n",
            "conv2_block3_1_bn 128\n",
            "conv2_block3_1_relu 0\n",
            "conv2_block3_2_conv 115605504\n",
            "conv2_block3_2_bn 128\n",
            "conv2_block3_2_relu 0\n",
            "conv2_block3_3_conv 51380224\n",
            "conv2_block3_3_bn 512\n",
            "conv2_block3_add 0\n",
            "conv2_block3_out 0\n",
            "conv3_block1_1_conv 25690112\n",
            "conv3_block1_1_bn 256\n",
            "conv3_block1_1_relu 0\n",
            "conv3_block1_2_conv 115605504\n",
            "conv3_block1_2_bn 256\n",
            "conv3_block1_2_relu 0\n",
            "conv3_block1_0_conv 102760448\n",
            "conv3_block1_3_conv 51380224\n",
            "conv3_block1_0_bn 1024\n",
            "conv3_block1_3_bn 1024\n",
            "conv3_block1_add 0\n",
            "conv3_block1_out 0\n",
            "conv3_block2_1_conv 51380224\n",
            "conv3_block2_1_bn 256\n",
            "conv3_block2_1_relu 0\n",
            "conv3_block2_2_conv 115605504\n",
            "conv3_block2_2_bn 256\n",
            "conv3_block2_2_relu 0\n",
            "conv3_block2_3_conv 51380224\n",
            "conv3_block2_3_bn 1024\n",
            "conv3_block2_add 0\n",
            "conv3_block2_out 0\n",
            "conv3_block3_1_conv 51380224\n",
            "conv3_block3_1_bn 256\n",
            "conv3_block3_1_relu 0\n",
            "conv3_block3_2_conv 115605504\n",
            "conv3_block3_2_bn 256\n",
            "conv3_block3_2_relu 0\n",
            "conv3_block3_3_conv 51380224\n",
            "conv3_block3_3_bn 1024\n",
            "conv3_block3_add 0\n",
            "conv3_block3_out 0\n",
            "conv3_block4_1_conv 51380224\n",
            "conv3_block4_1_bn 256\n",
            "conv3_block4_1_relu 0\n",
            "conv3_block4_2_conv 115605504\n",
            "conv3_block4_2_bn 256\n",
            "conv3_block4_2_relu 0\n",
            "conv3_block4_3_conv 51380224\n",
            "conv3_block4_3_bn 1024\n",
            "conv3_block4_add 0\n",
            "conv3_block4_out 0\n",
            "conv4_block1_1_conv 25690112\n",
            "conv4_block1_1_bn 512\n",
            "conv4_block1_1_relu 0\n",
            "conv4_block1_2_conv 115605504\n",
            "conv4_block1_2_bn 512\n",
            "conv4_block1_2_relu 0\n",
            "conv4_block1_0_conv 102760448\n",
            "conv4_block1_3_conv 51380224\n",
            "conv4_block1_0_bn 2048\n",
            "conv4_block1_3_bn 2048\n",
            "conv4_block1_add 0\n",
            "conv4_block1_out 0\n",
            "conv4_block2_1_conv 51380224\n",
            "conv4_block2_1_bn 512\n",
            "conv4_block2_1_relu 0\n",
            "conv4_block2_2_conv 115605504\n",
            "conv4_block2_2_bn 512\n",
            "conv4_block2_2_relu 0\n",
            "conv4_block2_3_conv 51380224\n",
            "conv4_block2_3_bn 2048\n",
            "conv4_block2_add 0\n",
            "conv4_block2_out 0\n",
            "conv4_block3_1_conv 51380224\n",
            "conv4_block3_1_bn 512\n",
            "conv4_block3_1_relu 0\n",
            "conv4_block3_2_conv 115605504\n",
            "conv4_block3_2_bn 512\n",
            "conv4_block3_2_relu 0\n",
            "conv4_block3_3_conv 51380224\n",
            "conv4_block3_3_bn 2048\n",
            "conv4_block3_add 0\n",
            "conv4_block3_out 0\n",
            "conv4_block4_1_conv 51380224\n",
            "conv4_block4_1_bn 512\n",
            "conv4_block4_1_relu 0\n",
            "conv4_block4_2_conv 115605504\n",
            "conv4_block4_2_bn 512\n",
            "conv4_block4_2_relu 0\n",
            "conv4_block4_3_conv 51380224\n",
            "conv4_block4_3_bn 2048\n",
            "conv4_block4_add 0\n",
            "conv4_block4_out 0\n",
            "conv4_block5_1_conv 51380224\n",
            "conv4_block5_1_bn 512\n",
            "conv4_block5_1_relu 0\n",
            "conv4_block5_2_conv 115605504\n",
            "conv4_block5_2_bn 512\n",
            "conv4_block5_2_relu 0\n",
            "conv4_block5_3_conv 51380224\n",
            "conv4_block5_3_bn 2048\n",
            "conv4_block5_add 0\n",
            "conv4_block5_out 0\n",
            "conv4_block6_1_conv 51380224\n",
            "conv4_block6_1_bn 512\n",
            "conv4_block6_1_relu 0\n",
            "conv4_block6_2_conv 115605504\n",
            "conv4_block6_2_bn 512\n",
            "conv4_block6_2_relu 0\n",
            "conv4_block6_3_conv 51380224\n",
            "conv4_block6_3_bn 2048\n",
            "conv4_block6_add 0\n",
            "conv4_block6_out 0\n",
            "conv5_block1_1_conv 25690112\n",
            "conv5_block1_1_bn 1024\n",
            "conv5_block1_1_relu 0\n",
            "conv5_block1_2_conv 115605504\n",
            "conv5_block1_2_bn 1024\n",
            "conv5_block1_2_relu 0\n",
            "conv5_block1_0_conv 102760448\n",
            "conv5_block1_3_conv 51380224\n",
            "conv5_block1_0_bn 4096\n",
            "conv5_block1_3_bn 4096\n",
            "conv5_block1_add 0\n",
            "conv5_block1_out 0\n",
            "conv5_block2_1_conv 51380224\n",
            "conv5_block2_1_bn 1024\n",
            "conv5_block2_1_relu 0\n",
            "conv5_block2_2_conv 115605504\n",
            "conv5_block2_2_bn 1024\n",
            "conv5_block2_2_relu 0\n",
            "conv5_block2_3_conv 51380224\n",
            "conv5_block2_3_bn 4096\n",
            "conv5_block2_add 0\n",
            "conv5_block2_out 0\n",
            "conv5_block3_1_conv 51380224\n",
            "conv5_block3_1_bn 1024\n",
            "conv5_block3_1_relu 0\n",
            "conv5_block3_2_conv 115605504\n",
            "conv5_block3_2_bn 1024\n",
            "conv5_block3_2_relu 0\n",
            "conv5_block3_3_conv 51380224\n",
            "conv5_block3_3_bn 4096\n",
            "conv5_block3_add 0\n",
            "conv5_block3_out 0\n",
            "avg_pool 0\n",
            "predictions 2049000\n",
            "123514678528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard\n"
      ],
      "metadata": {
        "id": "6xRYO57qAzOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext tensorboard\n",
        "# !kill 24824\n",
        "%tensorboard --logdir logs/train\n",
        "\n",
        "# %reload_ext tensorboard\n"
      ],
      "metadata": {
        "id": "490qctxTBcSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Model Summary"
      ],
      "metadata": {
        "id": "lMga9u2O_6BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.applications.resnet.ResNet50(weights='imagenet');\n",
        "# model.summary();\n",
        "len(model.layers)\n",
        "trainableLayerNameList = [layer.name for layer in model.layers if layer.trainable_variables]\n",
        "print(len(trainableLayerNameList));"
      ],
      "metadata": {
        "id": "_KhiPXEdPTXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable = 0\n",
        "\n",
        "# iterate over the layers in the model\n",
        "for layer in model.layers:\n",
        "    # check if the layer has trainable parameters\n",
        "    if layer.trainable_variables:\n",
        "        # if the layer has trainable parameters, increment the counter\n",
        "        trainable += 1\n",
        "\n",
        "# print the number of layers involved in the AllReduce operation\n",
        "print(f'Number of layers with trainable parameters: {trainable}')"
      ],
      "metadata": {
        "id": "ybE7ry0qP1pz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed9d609-c4cd-4b71-e318-840ef9ff71c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers with trainable parameters: 107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allreduce_layers = 0;\n",
        "for layer in model.layers:\n",
        "    # check if the layer has trainable parameters\n",
        "    if not layer.trainable_variables:\n",
        "        # if the layer has trainable parameters, increment the counter\n",
        "        if \"add\" in layer.name:\n",
        "          allreduce_layers += 1\n",
        "          \n",
        "# print the number of layers involved in the AllReduce operation\n",
        "print(f'Number of layers not involved in AllReduce: {allreduce_layers}')\n",
        "print(f'Number of layers involved in AllReduce: {len(model.layers) - allreduce_layers}')"
      ],
      "metadata": {
        "id": "gCohVvkkRyF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "resnet101 = tf.keras.applications.resnet.ResNet101(weights='imagenet')\n",
        "len(resnet101.layers)\n",
        "trainableLayerNameList = [layer.name for layer in resnet101.layers if layer.trainable_variables]\n",
        "print(len(trainableLayerNameList));\n",
        "allreduce_layers = 0;\n",
        "for layer in resnet101.layers:\n",
        "    # check if the layer has trainable parameters\n",
        "    if not layer.trainable_variables:\n",
        "        if \"add\" in layer.name:\n",
        "          allreduce_layers += 1\n",
        "          # print(layer.name);\n",
        "print(f'Number of layers not involved in AllReduce: {allreduce_layers}')"
      ],
      "metadata": {
        "id": "YQ-t2GJ_gcZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16 = tf.keras.applications.vgg16.VGG16(weights='imagenet')\n",
        "vgg16.summary();\n",
        "print(len(vgg16.layers))\n",
        "trainableLayerNameList = [layer.name for layer in vgg16.layers if layer.trainable_variables]\n",
        "print(len(trainableLayerNameList));\n",
        "allreduce_layers = 0;\n",
        "for layer in vgg16.layers:\n",
        "    # check if the layer has trainable parameters\n",
        "    if not layer.trainable_variables:\n",
        "        if \"add\" in layer.name:\n",
        "          allreduce_layers += 1\n",
        "          # print(layer.name);\n",
        "print(f'Number of layers not involved in AllReduce: {allreduce_layers}')"
      ],
      "metadata": {
        "id": "5Yla5IXykM7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "conv2d = 0;\n",
        "for layer in vgg16.layers:\n",
        "  if \"_conv\" in layer.name:\n",
        "    conv2d += 1;\n",
        "print(conv2d);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFoMUcturFkA",
        "outputId": "b2bf6d59-6d63-4122-bfa0-1d3d8a260f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainableLayerNameList)"
      ],
      "metadata": {
        "id": "zynCYzoFy_VV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}