{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4lVdGWjb2nU"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignment by selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS2oMufeb64g"
      },
      "source": [
        "# Part 4: Connecting Text and Images with CLIP\n",
        "\n",
        "Acknowledgement: This notebook is based on the code from https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb. Credit to \n",
        "OpenAI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k243qWV0ckpT"
      },
      "source": [
        "## Section I: Interacting with CLIP [0 pts]\n",
        "\n",
        "This is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications. The next cells will install the clip package and its dependencies, and check if PyTorch 1.7.1 or later is installed.\n",
        "\n",
        "__Note__: Although these steps are done for you, please take a moment to look through them and make sure you understand their purpose! Understanding how CLIP works will help you solve Section II."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra4JaYNhctzV"
      },
      "outputs": [],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SHbTOTtcyyM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "torch_version = torch.__version__.split(\".\")\n",
        "assert (int(torch_version[0]) == 1 and int(torch_version[1]) >=7) or int(torch_version[0]) > 1, \"PyTorch 1.7.1 or later is required\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ_pLusic71v"
      },
      "source": [
        "### Loading the model\n",
        "\n",
        "`clip.available_models()` will list the names of available CLIP models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YcRI-G4c9qw"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvnA8SUTdCLm"
      },
      "outputs": [],
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mINCZx6dHyV"
      },
      "source": [
        "### Image Preprocessing\n",
        "\n",
        "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
        "\n",
        "The second return value from `clip.load()` contains a torchvision `Transform` that performs this preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMhXZDymdLSf"
      },
      "outputs": [],
      "source": [
        "preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhlQSgaVdTNd"
      },
      "source": [
        "### Text Preprocessing\n",
        "\n",
        "We use a case-insensitive tokenizer, which can be invoked using `clip.tokenize()`. By default, the outputs are padded to become 77 tokens long, which is what the CLIP models expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Bj_5pmmdYbS"
      },
      "outputs": [],
      "source": [
        "clip.tokenize(\"Hello World!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgkekdIUM0Mk"
      },
      "source": [
        "### Setting up input images and texts\n",
        "\n",
        "We are going to feed 8 example images and their textual descriptions to the model, and compare the similarity between the corresponding features.\n",
        "\n",
        "The tokenizer is case-insensitive, and we can freely give any suitable textual descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTzpuNvPM4ZM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# images in skimage to use and their textual descriptions\n",
        "descriptions = {\n",
        "    \"page\": \"a page of text about segmentation\",\n",
        "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
        "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
        "    \"rocket\": \"a rocket standing on a launchpad\",\n",
        "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
        "    \"camera\": \"a person looking at a camera on a tripod\",\n",
        "    \"horse\": \"a black-and-white silhouette of a horse\", \n",
        "    \"coffee\": \"a cup of coffee on a saucer\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Mk855_SM7iv"
      },
      "outputs": [],
      "source": [
        "original_images = []\n",
        "images = []\n",
        "texts = []\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
        "    name = os.path.splitext(filename)[0]\n",
        "    if name not in descriptions:\n",
        "        continue\n",
        "    \n",
        "    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n",
        "\n",
        "    plt.subplot(2, 4, len(images) + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"{filename}\\n{descriptions[name]}\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    original_images.append(image)\n",
        "    images.append(preprocess(image))\n",
        "    texts.append(descriptions[name])\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yGO91x2M-E_"
      },
      "source": [
        "### Building features\n",
        "\n",
        "We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZceaA-FNBbo"
      },
      "outputs": [],
      "source": [
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdnSyQcwNDT5"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    text_features = model.encode_text(text_tokens).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS09uzL7NHpW"
      },
      "source": [
        "### Calculating cosine similarity\n",
        "\n",
        "We normalize the features and calculate the dot product of each pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79G5WDMHNNkC"
      },
      "outputs": [],
      "source": [
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2KSRXn-NQKy"
      },
      "outputs": [],
      "source": [
        "count = len(descriptions)\n",
        "\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "plt.yticks(range(count), texts, fontsize=18)\n",
        "plt.xticks([])\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "  plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, count - 0.5])\n",
        "plt.ylim([count + 0.5, -2])\n",
        "\n",
        "plt.title(\"Cosine similarity between text and image features\", size=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOJcirVSNW8v"
      },
      "source": [
        "### Zero-Shot Image Classification\n",
        "\n",
        "You can classify images using the cosine similarity (times 100) as the logits to the softmax operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOeXZ8whNbF6"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "cifar100 = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtdOIk-8Nik2"
      },
      "outputs": [],
      "source": [
        "text_descriptions = [f\"This is a photo of a  {label}\" for label in cifar100.classes]\n",
        "text_tokens = clip.tokenize(text_descriptions).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcU94r8LNkeQ"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_Xn9NT1Nnqw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 16))\n",
        "\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.subplot(4, 4, 2 * i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(4, 4, 2 * i + 2)\n",
        "    y = np.arange(top_probs.shape[-1])\n",
        "    plt.grid()\n",
        "    plt.barh(y, top_probs[i])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.gca().set_axisbelow(True)\n",
        "    plt.yticks(y, [cifar100.classes[index] for index in top_labels[i].numpy()])\n",
        "    plt.xlabel(\"probability\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU9iToj8sCCJ"
      },
      "source": [
        "## Section II: Let's do a Scavenger Hunt! [1 pt]\n",
        "\n",
        "Now, lets see if we can write a caption to retrieve a particular image using CLIP. Specifically, your task is to __write a caption that best matches the following image__:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1KxvxIO_2HIT7H6WWTwApz7OucmcYah3k\" height=256px>\n",
        "\n",
        "You can think of your caption as a _prompt_ to the model, designed to retrieve the above image from a large collection of images. As you will see, getting this prompt correct can be tricky! To test your caption, we will use it to retrieve images from ImageNet with CLIP. You should continue to tweak your caption until the above image is returned!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lZqr1BPsbMJ"
      },
      "source": [
        "First, we will download a subset of ImageNet called _\"Tiny ImageNet\"_. Tiny ImageNet has only 200 classes, with each class having 500 traininig images, 50 validation images and 50 test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhkxW3XSqEr5"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/seshuad/IMagenet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2_SKdvisoef"
      },
      "source": [
        "In order to reduce time & memory consumption, we will consider only __one__ category of images from the train set (can you guess what this category corresponds to?) Run the following cell to load these images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKRixiWmb_Ug"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "img_paths = []\n",
        "for rootdir, subdir, filenames in os.walk(\"/content/IMagenet/tiny-imagenet-200/train/n09256479/images\"):\n",
        "    for file_ in filenames:\n",
        "        if not file_.endswith(\".JPEG\"):\n",
        "            continue\n",
        "        img_paths.append(os.path.join(rootdir, file_))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__[1pt] Write your caption below.__ Remember, you should think of this caption like a _prompt_; you want to write a caption that will best match the above image under the CLIP model.\n",
        "\n",
        "> __Hint__: Be specific! This works best when your caption is short, but descriptive and specific."
      ],
      "metadata": {
        "id": "NlWfF-urTadS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9yw5gGRgmAm"
      },
      "outputs": [],
      "source": [
        "caption = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwQlYxfDxLSS"
      },
      "source": [
        "Below, we will display the image that produces the highest probability under the model given your caption. You should continue to tweak your caption until the target image (see above) is produced!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTmZio-qiWdc"
      },
      "outputs": [],
      "source": [
        "original_images = []\n",
        "images = []\n",
        "\n",
        "for img_path in img_paths:\n",
        "\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    original_images.append(image)\n",
        "    images.append(preprocess(image))\n",
        "    \n",
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# Delete these to avoid consuming up too much memory\n",
        "del images\n",
        "del image_input\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "text_tokens = clip.tokenize(caption).cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=0).cpu().detach().numpy()\n",
        "highest_prob = np.argmax(text_probs)\n",
        "plt.axis('off')\n",
        "plt.imshow(original_images[highest_prob])\n",
        "\n",
        "print(img_paths[highest_prob])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l4lVdGWjb2nU"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}