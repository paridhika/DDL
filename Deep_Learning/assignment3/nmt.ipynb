{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignment by selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "source": [
        "# Setup PyTorch\n",
        "\n",
        "All files will be stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-6MQhMOlHXD"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install Pillow\n",
        "%mkdir -p ./content/csc421/a3/\n",
        "%cd ./content/csc421/a3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "from pathlib import Path\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def get_file(\n",
        "    fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"\n",
        "):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + \".tar.gz\"\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "\n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print(\"Downloading data from\", origin)\n",
        "\n",
        "        error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print(\"Extracting file.\")\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "    Arguments:\n",
        "        tensor: A Tensor object.\n",
        "        cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "    Returns:\n",
        "        A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title(\"BS={}, nhid={}\".format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel(\"Epochs\", fontsize=16)\n",
        "    plt.ylabel(\"Loss\", fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, \"loss_plot.pdf\"))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_gru(l1, l2, o1, o2, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and val loss curves from GRU runs.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    ax[0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0].title.set_text(\"Train Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
        "    ax[1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
        "    ax[1].title.set_text(\"Val Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
        "\n",
        "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"GRU Performance by Dataset\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.85)\n",
        "    plt.legend()\n",
        "\n",
        "    plt_path = \"./loss_plot_{}.pdf\".format(fn)\n",
        "    plt.savefig(plt_path)\n",
        "    print(f\"Plot saved to: {Path(plt_path).resolve()}\")\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
        "\n",
        "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0][0].title.set_text(\n",
        "        \"Train Loss | Model Hidden Size = {}\".format(o1.hidden_size)\n",
        "    )\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0][1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o1.hidden_size))\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label=\"ds=\" + o3.data_file_name)\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"ds=\" + o4.data_file_name)\n",
        "    ax[1][0].title.set_text(\n",
        "        \"Train Loss | Model Hidden Size = {}\".format(o3.hidden_size)\n",
        "    )\n",
        "\n",
        "    ax[1][1].plot(range(len(l3[1])), l3[1], label=\"ds=\" + o3.data_file_name)\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"ds=\" + o4.data_file_name)\n",
        "    ax[1][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o4.hidden_size))\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
        "    # plt.close()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
        "\n",
        "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"hid_size=\" + str(o1.hidden_size))\n",
        "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label=\"hid_size=\" + str(o3.hidden_size))\n",
        "    ax[0][0].title.set_text(\"Train Loss | Dataset = \" + o1.data_file_name)\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"hid_size=\" + str(o1.hidden_size))\n",
        "    ax[0][1].plot(range(len(l3[1])), l3[1], label=\"hid_size=\" + str(o3.hidden_size))\n",
        "    ax[0][1].title.set_text(\"Val Loss | Dataset = \" + o1.data_file_name)\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label=\"hid_size=\" + str(o2.hidden_size))\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"hid_size=\" + str(o4.hidden_size))\n",
        "    ax[1][0].title.set_text(\"Train Loss | Dataset = \" + o3.data_file_name)\n",
        "\n",
        "    ax[1][1].plot(range(len(l2[1])), l2[1], label=\"hid_size=\" + str(o2.hidden_size))\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"hid_size=\" + str(o4.hidden_size))\n",
        "    ax[1][1].title.set_text(\"Val Loss | Dataset = \" + o4.data_file_name)\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
        "    # plt.close()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, \"encoder.pt\"), \"wb\") as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, \"decoder.pt\"), \"wb\") as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, \"idx_dict.pkl\"), \"wb\") as f:\n",
        "        pkl.dump(idx_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "outputs": [],
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\"\"\"\n",
        "    lines = open(filename).read().strip().lower().split(\"\\n\")\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\"\"\"\n",
        "    return all(c.isalpha() or c == \"-\" for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data(file_name):\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\"\"\"\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\"\"\"\n",
        "    path = \"./data/{}.txt\".format(file_name)\n",
        "    source_lines, target_lines = read_pairs(path)\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(\"\".join(source_lines)) | set(\"\".join(target_lines))\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = {\n",
        "        char: index for (index, char) in enumerate(sorted(list(all_characters)))\n",
        "    }\n",
        "\n",
        "    # Add start, end, and  tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    end_prompt_token = len(char_to_index) + 2\n",
        "    char_to_index[\"SOS\"] = start_token\n",
        "    char_to_index[\"EOS\"] = end_token\n",
        "    char_to_index[\"EOP\"] = end_prompt_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = {index: char for (char, index) in char_to_index.items()}\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "    # import pdb; pdb.set_trace()\n",
        "    idx_dict = {\n",
        "        \"char_to_index\": char_to_index,\n",
        "        \"index_to_char\": index_to_char,\n",
        "        \"start_token\": start_token,\n",
        "        \"end_token\": end_token,\n",
        "        \"end_prompt_token\": end_prompt_token\n",
        "    }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s, t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s, t))\n",
        "\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aROU2xZanDKq"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(\n",
        "    fname=\"pig_latin_small.txt\",\n",
        "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_small.txt\",\n",
        "    untar=False,\n",
        ")\n",
        "\n",
        "data_fpath = get_file(\n",
        "    fname=\"pig_latin_large.txt\",\n",
        "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_large.txt\",\n",
        "    untar=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq"
      },
      "source": [
        "# Part 1: Scaled Dot Production Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\"\"\"\n",
        "    return [char_to_index[char] for char in s] + [\n",
        "        end_token\n",
        "    ]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    return \" \".join(\n",
        "        [translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()]\n",
        "    )\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\"\"\"\n",
        "\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "    index_to_char = idx_dict[\"index_to_char\"]\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = \"\"\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(\n",
        "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
        "    )  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(\n",
        "            decoder_inputs, encoder_annotations, decoder_hidden\n",
        "        )\n",
        "\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1]  # latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [\n",
        "                    index_to_char[int(item)]\n",
        "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\"\"\"\n",
        "    if idx_dict is None:\n",
        "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "    index_to_char = idx_dict[\"index_to_char\"]\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = \"\"\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(\n",
        "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
        "    )  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(\n",
        "            decoder_inputs, encoder_annotations, decoder_hidden\n",
        "        )\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1]  # latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [\n",
        "                    index_to_char[int(item)]\n",
        "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    if isinstance(attention_weights, tuple):\n",
        "        ## transformer's attention mweights\n",
        "        attention_weights, self_attention_weights = attention_weights\n",
        "\n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "\n",
        "    for i in range(len(all_attention_weights)):\n",
        "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(attention_weights_matrix, cmap=\"bone\")\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_yticklabels([\"\"] + list(input_string) + [\"EOS\"], rotation=90)\n",
        "        ax.set_xticklabels(\n",
        "            [\"\"] + list(gen_string) + ([\"EOS\"] if produced_end_token else [])\n",
        "        )\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        # Add title\n",
        "        plt.xlabel(\"Attention weights to the source sentence in layer {}\".format(i + 1))\n",
        "        plt.tight_layout()\n",
        "        plt.grid(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in input_strings\n",
        "        ]\n",
        "        target_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in target_strings\n",
        "        ]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = (\n",
        "                torch.ones(BS).long().unsqueeze(1) * start_token\n",
        "            )  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat(\n",
        "                [decoder_input, targets[:, 0:-1]], dim=1\n",
        "            )  # Gets decoder inputs by shifting the targets to the right\n",
        "\n",
        "            decoder_outputs, attention_weights = decoder(\n",
        "                decoder_inputs, encoder_annotations, decoder_hidden\n",
        "            )\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "                # Zero gradients\n",
        "                optimizer.zero_grad()\n",
        "                # Compute gradients\n",
        "                loss.backward()\n",
        "                # Update the parameters of the encoder and decoder\n",
        "                optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "def training_loop(\n",
        "    train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, \"loss_log.txt\"), \"w\")\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0][\"lr\"] *= opts.lr_decay\n",
        "\n",
        "        train_loss = compute_loss(\n",
        "            train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts\n",
        "        )\n",
        "        val_loss = compute_loss(\n",
        "            val_dict, encoder, decoder, idx_dict, criterion, None, opts\n",
        "        )\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\n",
        "                \"Validation loss has not improved in {} epochs, stopping early\".format(\n",
        "                    opts.early_stopping_patience\n",
        "                )\n",
        "            )\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (train_losses, mean_val_losses)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\n",
        "            \"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(\n",
        "                epoch, mean_train_loss, mean_val_loss, gen_string\n",
        "            )\n",
        "        )\n",
        "\n",
        "        loss_log.write(\"{} {} {}\\n\".format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "\n",
        "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
        "\n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    return (train_losses, mean_val_losses)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Data Stats\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print(\"Num unique word pairs: {}\".format(len(line_pairs)))\n",
        "    print(\"Vocabulary: {}\".format(idx_dict[\"char_to_index\"].keys()))\n",
        "    print(\"Vocab size: {}\".format(vocab_size))\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    encoder = TransformerEncoder(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_size=opts.hidden_size,\n",
        "        num_layers=opts.num_transformer_layers,\n",
        "        opts=opts,\n",
        "    )\n",
        "\n",
        "    decoder = TransformerDecoder(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_size=opts.hidden_size,\n",
        "        num_layers=opts.num_transformer_layers,\n",
        "    )\n",
        "\n",
        "    #### setup checkpoint path\n",
        "    model_name = \"encoder-decoder-h{}-bs{}-{}\".format(\n",
        "        opts.hidden_size, opts.batch_size, opts.data_file_name\n",
        "    )\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(\n",
        "        list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(\n",
        "            train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Exiting early from training.\")\n",
        "        return encoder, decoder, losses\n",
        "\n",
        "    return encoder, decoder, losses\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Opts\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print(\"{:>30}: {:<30}\".format(key, opts.__dict__[key]).center(80))\n",
        "    print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "XJ09iC85pGjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Implement scaled dot-product attention\n",
        "\n",
        "In the next cell, you will implement the [scaled dot-product attention](https://paperswithcode.com/method/scaled) mechanism. See the assignment handouts for details."
      ],
      "metadata": {
        "id": "FmPIeIrmm2BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "        self.scaling_factor = torch.rsqrt(\n",
        "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # batch_size = ...\n",
        "        # q = ...\n",
        "        # k = ...\n",
        "        # v = ...\n",
        "        # unnormalized_attention = ...\n",
        "        # attention_weights = ...\n",
        "        # context = ...\n",
        "        # return context, attention_weights\n",
        "        return context, attention_weights"
      ],
      "metadata": {
        "id": "XLumQdpOm-Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113"
      },
      "source": [
        "## Step 2: Implement causal dot-product Attention\n",
        "\n",
        "\n",
        "Now, implement the casual scaled dot-product attention mechanism. It will be very similar to your implementation for `ScaledDotAttention`. The additional step is to mask out the attention to future timesteps so this attention mechanism can be used in a decoder. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovigzQffrKqj"
      },
      "outputs": [],
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "        self.scaling_factor = torch.rsqrt(\n",
        "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # batch_size = \n",
        "        # q = \n",
        "        # k = \n",
        "        # v = \n",
        "        # unnormalized_attention = \n",
        "        # mask = \n",
        "        # attention_weights = \n",
        "        # context = \n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Transformer encoder and decoder\n",
        "\n",
        "The following cells provide an implementation of the transformer encoder and decoder that use your `ScaledDotAttention` and `CausalScaledDotAttention`. Please read through them to understand what they are doing."
      ],
      "metadata": {
        "id": "NPKHAiPQmOAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attentions = nn.ModuleList(\n",
        "            [\n",
        "                ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_mlps = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(hidden_size, hidden_size),\n",
        "                    nn.ReLU(),\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        annotations = encoded\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](\n",
        "                annotations, annotations, annotations\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            annotations = residual_annotations + new_annotations\n",
        "\n",
        "        # Transformer encoder does not have a last hidden or cell layer.\n",
        "        return annotations, None\n",
        "        # return annotations, None, None\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
        "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        if self.opts.cuda:\n",
        "            pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings"
      ],
      "metadata": {
        "id": "g4wePP1smlzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.self_attentions = nn.ModuleList(\n",
        "            [\n",
        "                CausalScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.encoder_attentions = nn.ModuleList(\n",
        "            [\n",
        "                ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_mlps = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(hidden_size, hidden_size),\n",
        "                    nn.ReLU(),\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        embed = embed + self.positional_encodings[:seq_len]\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](\n",
        "                contexts, contexts, contexts\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](\n",
        "                residual_contexts, annotations, annotations\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "\n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "\n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "\n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
        "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings"
      ],
      "metadata": {
        "id": "74W3W8AomoQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 4: Training and analysis (with scaled dot-product attention)\n",
        "\n",
        "Now we will train a (simplified) transformer encoder-decoder model.\n",
        "\n",
        "First, we train our smaller model on the small dataset. Use this model to answer Question 4 in the handout."
      ],
      "metadata": {
        "id": "Pbp1XQCp882O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans32_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": use_cuda,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"num_transformer_layers\": 4,\n",
        "}\n",
        "\n",
        "\n",
        "trans32_args_s.update(args_dict)\n",
        "print_opts(trans32_args_s)\n",
        "\n",
        "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "metadata": {
        "id": "FKIgJkJnpYWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "metadata": {
        "id": "gceeSUdy85W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
      ],
      "metadata": {
        "id": "gebtzekR9PL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans32_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 10,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 32,\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans32_args_l.update(args_dict)\n",
        "print_opts(trans32_args_l)\n",
        "\n",
        "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "metadata": {
        "id": "YPGKZrbJ9Q0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans64_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 64,  # Increased model size\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans64_args_s.update(args_dict)\n",
        "print_opts(trans64_args_s)\n",
        "\n",
        "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "metadata": {
        "id": "vGR-QZo99S6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans64_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 64,  # Increased model size\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans64_args_l.update(args_dict)\n",
        "print_opts(trans64_args_l)\n",
        "\n",
        "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "metadata": {
        "id": "Ql_bggkf9Umx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results.\n"
      ],
      "metadata": {
        "id": "MenyRIr78qm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_loss_comparison_by_dataset(\n",
        "    trans32_losses_s,\n",
        "    trans32_losses_l,\n",
        "    trans64_losses_s,\n",
        "    trans64_losses_l,\n",
        "    trans32_args_s,\n",
        "    trans32_args_l,\n",
        "    trans64_args_s,\n",
        "    trans64_args_l,\n",
        "    \"trans_by_dataset\",\n",
        ")\n",
        "save_loss_comparison_by_hidden(\n",
        "    trans32_losses_s,\n",
        "    trans32_losses_l,\n",
        "    trans64_losses_s,\n",
        "    trans64_losses_l,\n",
        "    trans32_args_s,\n",
        "    trans32_args_l,\n",
        "    trans64_args_s,\n",
        "    trans64_args_l,\n",
        "    \"trans_by_hidden\",\n",
        ")"
      ],
      "metadata": {
        "id": "iFQ40R6b8vPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Decoder-only NMT"
      ],
      "metadata": {
        "id": "KFuyoFLXjG4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and evaluation code"
      ],
      "metadata": {
        "id": "rEt6SplHo7NV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa5-onJhoSeM"
      },
      "outputs": [],
      "source": [
        "# Get Decoder Params\n",
        "def transformer_params(vocab_size, hidden_size, num_layers, max_seq_len=1000):\n",
        "    \"\"\" Given transformerEncoder config calculate total number of parameters \"\"\"\n",
        "    # token and position embeddings\n",
        "    embeddings = vocab_size * hidden_size + max_seq_len * hidden_size\n",
        "    # attention\n",
        "    self_attention = 3 * hidden_size**2 + 3 * hidden_size\n",
        "    attention_mlp = hidden_size**2 + hidden_size\n",
        "    # out\n",
        "    out = hidden_size * vocab_size + vocab_size\n",
        "    # note: embeddings are not included in the param count!\n",
        "    total_params = num_layers * (self_attention + attention_mlp) + out\n",
        "    return total_params\n",
        "\n",
        "\n",
        "# Let us use a factor of 2 to describe the multiply accumulate cost. \n",
        "# See more details in https://arxiv.org/pdf/2203.15556.pdf Appendix F\n",
        "# Get Decoder Flops\n",
        "def transformer_flops(seq_len, vocab_size, hidden_size, num_layers):\n",
        "    \"\"\" Given AttentionEncoder config calculate total number of parameters \"\"\"\n",
        "    # token and position embeddings\n",
        "    embeddings = 2 * seq_len * vocab_size * hidden_size\n",
        "    # attention\n",
        "    # key, query, value projections\n",
        "    attention = 2 * 3 * seq_len * hidden_size * hidden_size\n",
        "    # key @ query logits\n",
        "    attlogits = 2 * seq_len * seq_len * hidden_size\n",
        "    # softmax\n",
        "    attsoftmax = 3 * seq_len * seq_len # 3 * is for subtract (max), exp, divide (?)\n",
        "    # softmax @ value reductions\n",
        "    attvalue = 2 * seq_len * seq_len * hidden_size\n",
        "    # mlp\n",
        "    attlinear = 2 * seq_len * hidden_size * hidden_size\n",
        "    att = attention + attlogits + attsoftmax + attvalue + attlinear\n",
        "\n",
        "    # out\n",
        "    out = 2 * seq_len * vocab_size * hidden_size\n",
        "\n",
        "    # note: we ignore the embedding for now\n",
        "    forward_flops = num_layers * att + out\n",
        "    backward_flops = 2 * forward_flops # as in Kaplan et al. 2020\n",
        "    total_flops = forward_flops + backward_flops\n",
        "    return total_flops\n",
        "\n",
        "def string_to_index_list(s, char_to_index, end_token=None):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\"\"\"\n",
        "    # return [char_to_index[char] for char in s] + [\n",
        "    #     end_token\n",
        "    # ]  # Adds the end token to each index list\n",
        "\n",
        "    if end_token is not None:\n",
        "        return [char_to_index[char] for char in s] + [\n",
        "            end_token\n",
        "        ]  # Adds the end token to each index list\n",
        "    else:\n",
        "        return [char_to_index[char] for char in s]\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    return \" \".join(\n",
        "        [translate(word, decoder, idx_dict, opts) for word in sentence.split()]\n",
        "    )\n",
        "\n",
        "\n",
        "def translate(input_string, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\"\"\"\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "    index_to_char = idx_dict[\"index_to_char\"]\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    end_prompt_token = idx_dict[\"end_prompt_token\"]\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = \"\"\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_prompt_token)\n",
        "    indexes = to_var(\n",
        "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
        "    )  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    # decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_input = torch.cat([decoder_input, indexes], dim=1)\n",
        "\n",
        "    decoder_inputs = decoder_input\n",
        "    out_start_idx = len(decoder_inputs[0]) - 1\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(\n",
        "            decoder_inputs\n",
        "        )\n",
        "\n",
        "        generated_words_all = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        generated_words = generated_words_all[:, out_start_idx:]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1]  # latest output token\n",
        "      \n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [\n",
        "                    index_to_char[int(item)]\n",
        "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\"\"\"\n",
        "    if idx_dict is None:\n",
        "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "    index_to_char = idx_dict[\"index_to_char\"]\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = \"\"\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(\n",
        "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
        "    )  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(\n",
        "            decoder_inputs, encoder_annotations, decoder_hidden\n",
        "        )\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1]  # latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [\n",
        "                    index_to_char[int(item)]\n",
        "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    if isinstance(attention_weights, tuple):\n",
        "        ## transformer's attention mweights\n",
        "        attention_weights, self_attention_weights = attention_weights\n",
        "\n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "\n",
        "    for i in range(len(all_attention_weights)):\n",
        "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(attention_weights_matrix, cmap=\"bone\")\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_yticklabels([\"\"] + list(input_string) + [\"EOS\"], rotation=90)\n",
        "        ax.set_xticklabels(\n",
        "            [\"\"] + list(gen_string) + ([\"EOS\"] if produced_end_token else [])\n",
        "        )\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        # Add title\n",
        "        plt.xlabel(\"Attention weights to the source sentence in layer {}\".format(i + 1))\n",
        "        plt.tight_layout()\n",
        "        plt.grid(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "def compute_loss(data_dict, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    end_prompt_token = idx_dict[\"end_prompt_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    losses = []\n",
        "    num_tokens = 0\n",
        "    num_flops = 0\n",
        "    num_example = 0\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_prompt_token))\n",
        "            for s in input_strings\n",
        "        ]\n",
        "        target_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in target_strings\n",
        "        ]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            src_EOP = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            tgt_EOS = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "            tgt_EOS_len = tgt_EOS.size(1)  \n",
        "\n",
        "            SOS_src_EOP_tgt, src_EOP_tgt_EOS = generate_tensors_for_training_decoder_nmt(src_EOP, tgt_EOS, start_token, opts.cuda)\n",
        "\n",
        "            predict_src_EOP_tgt_EOS, attention_weights = decoder(SOS_src_EOP_tgt)\n",
        "\n",
        "            predict_tgt_EOS = predict_src_EOP_tgt_EOS[:, -tgt_EOS_len:, :]\n",
        "            predict_tgt_EOS_flatten = predict_tgt_EOS.reshape(-1, predict_tgt_EOS.size(2))\n",
        "            tgt_EOS_flatten = tgt_EOS.view(-1)\n",
        "            loss = criterion(predict_tgt_EOS_flatten, tgt_EOS_flatten)\n",
        "           \n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # Collect some statistics\n",
        "            BS = src_EOP.size(0) \n",
        "            i_seq_len = SOS_src_EOP_tgt.size(1) \n",
        "            num_tokens += BS * i_seq_len\n",
        "            num_flops += BS * transformer_flops(i_seq_len, opts.vocab_size, opts.hidden_size, opts.num_layers)\n",
        "            num_example += BS\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "                # Zero gradients\n",
        "                optimizer.zero_grad()\n",
        "                # Compute gradients\n",
        "                loss.backward()\n",
        "                # Update the parameters of the encoder and decoder\n",
        "                optimizer.step()\n",
        "\n",
        "    return losses, num_tokens, num_flops, num_example\n",
        "\n",
        "\n",
        "def training_loop(\n",
        "    train_dict, val_dict, idx_dict, decoder, criterion, optimizer, opts\n",
        "):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, \"loss_log.txt\"), \"w\")\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    num_tokens = []\n",
        "    num_flops = []\n",
        "    num_examples = []\n",
        "    num_params = []\n",
        "\n",
        "    total_tokens = 0\n",
        "    total_flops = 0\n",
        "    total_examples = 0\n",
        "    params_count = transformer_params(opts.vocab_size, opts.hidden_size, opts.num_layers)\n",
        "\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "        optimizer.param_groups[0][\"lr\"] *= opts.lr_decay\n",
        "\n",
        "        train_loss, num_token, num_flop, num_example = compute_loss(\n",
        "            train_dict, decoder, idx_dict, criterion, optimizer, opts\n",
        "        )\n",
        "        val_loss, _, _, _ = compute_loss(\n",
        "            val_dict, decoder, idx_dict, criterion, None, opts\n",
        "        )\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            checkpoint(decoder, idx_dict, opts)\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\n",
        "                \"Validation loss has not improved in {} epochs, stopping early\".format(\n",
        "                    opts.early_stopping_patience\n",
        "                )\n",
        "            )\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (mean_train_losses, mean_val_losses, num_tokens, num_flops, num_examples, num_params)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, decoder, idx_dict, opts)\n",
        "        total_tokens += num_token\n",
        "        total_flops += num_flop\n",
        "        total_examples += num_example\n",
        "\n",
        "        print(\n",
        "            \"Epoch: {:3d} | Flops (T): {}, Tokens (M) {}, Train Loss {:.3f}, Val Loss {:.3f} | Gen: {:20s}\".format(\n",
        "                epoch, total_flops/1e12, total_tokens/1e6,\n",
        "                mean_train_loss, mean_val_loss, gen_string)\n",
        "        )\n",
        "\n",
        "        loss_log.write(\"{} {} {}\\n\".format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "        num_tokens.append(total_tokens)\n",
        "        num_flops.append(total_flops)\n",
        "        num_examples.append(total_examples)\n",
        "        num_params.append(params_count)\n",
        "        \n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    print(\"Avg tokens per example (Seq Len): {}, Avg flops per example: {}\".format(\n",
        "        total_tokens/total_examples, total_flops/total_examples))\n",
        "    return (mean_train_losses, mean_val_losses, num_tokens, num_flops, num_examples, num_params)\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "    print(train_dict)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    decoder = TransformerDecoder(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_size=opts.hidden_size,\n",
        "        num_layers=opts.num_transformer_layers,\n",
        "        loss_seq_span=opts.loss_seq_span\n",
        "    )\n",
        "\n",
        "    #### setup checkpoint path\n",
        "    model_name = \"decoder-only-h{}-bs{}-{}\".format(\n",
        "        opts.hidden_size, opts.batch_size, opts.data_file_name\n",
        "    )\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(\n",
        "        list(decoder.parameters()), lr=opts.learning_rate\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(\n",
        "            train_dict, val_dict, idx_dict, decoder, criterion, optimizer, opts\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Exiting early from training.\")\n",
        "        return decoder, losses\n",
        "\n",
        "    return decoder, losses, idx_dict\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Data Stats\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print(\"Num unique word pairs: {}\".format(len(line_pairs)))\n",
        "    print(\"Vocabulary: {}\".format(idx_dict[\"char_to_index\"].keys()))\n",
        "    print(\"Vocab size: {}\".format(vocab_size))\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Opts\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print(\"{:>30}: {:<30}\".format(key, opts.__dict__[key]).center(80))\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "def checkpoint(decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the decoder model, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, \"decoder.pt\"), \"wb\") as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, \"idx_dict.pkl\"), \"wb\") as f:\n",
        "        pkl.dump(idx_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Construct the input tensors and the target tensors for training a decoder\n",
        "Implement the function below by following the three steps mentioned."
      ],
      "metadata": {
        "id": "ey_03SAJjRoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tensors_for_training_decoder_nmt(src_EOP, tgt_EOS, start_token, cuda):\n",
        "    # ------------\n",
        "    # FILL THIS IN\n",
        "    # ------------\n",
        "    # Step1: concatenate input_EOP, and target_EOS vectors to form a target tensor.\n",
        "    # src_EOP_tgt_EOS = \n",
        "    # Step2: make a sos vector\n",
        "    # sos_vector = \n",
        "    # sos_vector = to_var(sos_vector, cuda)\n",
        "    # Step3: make a concatenated input tensor to the decoder-only NMT (format: Start-of-token source end-of-prompt target)  \n",
        "    # SOS_src_EOP_tgt = \n",
        "    return SOS_src_EOP_tgt, src_EOP_tgt_EOS"
      ],
      "metadata": {
        "id": "TlUwi7c4jfG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi"
      },
      "source": [
        "## Step 2: Implement decoder forward pass\n",
        "\n",
        "The following cell defines the decoder only transformer.  You can check the forward function implementation from the encoder-decoder model ot get an idea of how to implement this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyvTZFxtrvc6"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, loss_seq_span):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.self_attentions = nn.ModuleList(\n",
        "            [\n",
        "                CausalScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        self.attention_mlps = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(hidden_size, hidden_size),\n",
        "                    nn.ReLU(),\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "        self.loss_seq_span = loss_seq_span\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        return output, self_attention_weights\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
        "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
        "        trig_args = pos_indices / (10000 ** exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Train the decoder-only model\n",
        "Run the following block to see the training and validation loss of a decoder-only transformer.  How is the quality of the generated translation?"
      ],
      "metadata": {
        "id": "nv6WDG36iYmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "trans_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\", #pig_latin_small debug\n",
        "    \"cuda\": use_cuda,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 128,\n",
        "    \"num_transformer_layers\": 4,\n",
        "    \"loss_seq_span\": \"target_only\",\n",
        "    \"num_layers\": 4,\n",
        "    \"vocab_size\": 30 \n",
        "}\n",
        "trans_args_s.update(args_dict)\n",
        "print_opts(trans_args_s)\n",
        "trans_decoder_s, trans_losses_s, idx_dict = train(trans_args_s)\n"
      ],
      "metadata": {
        "id": "9izJWq4ViX2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE,  trans_decoder_s, None, trans_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "metadata": {
        "id": "3nEmqvq2FQug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb"
      },
      "source": [
        "# Part 3: Compute Optimal Model\n",
        "\n",
        "Now we will train a set of decoder only transformer models to investigate the compute optimal model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Model training\n",
        "- Let's train six models of different model sizes."
      ],
      "metadata": {
        "id": "N20B2EKigj5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"floor\"\n",
        "trans16_args_s = AttrDict()\n",
        "args_dict_16 = {\n",
        "    \"data_file_name\": \"pig_latin_small\", #pig_latin_small debug\n",
        "    \"cuda\": use_cuda,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 16,\n",
        "    \"num_transformer_layers\": 4,\n",
        "    \"loss_seq_span\": \"target_only\",\n",
        "    \"num_layers\": 4,\n",
        "    \"vocab_size\": 30 \n",
        "}\n",
        "trans16_args_s.update(args_dict_16)\n",
        "print_opts(trans16_args_s)\n",
        "trans16_decoder_s, trans16_losses_s, idx_dict = train(trans16_args_s)"
      ],
      "metadata": {
        "id": "eeEEpA-gCtt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"floor\"\n",
        "trans32_args_s = AttrDict()\n",
        "args_dict_32 = {\n",
        "    \"data_file_name\": \"pig_latin_small\", #pig_latin_small debug\n",
        "    \"cuda\": use_cuda,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"num_transformer_layers\": 4,\n",
        "    \"loss_seq_span\": \"target_only\",\n",
        "    \"num_layers\": 4,\n",
        "    \"vocab_size\": 30 \n",
        "}\n",
        "trans32_args_s.update(args_dict_32)\n",
        "print_opts(trans32_args_s)\n",
        "trans32_decoder_s, trans32_losses_s, idx_dict = train(trans32_args_s)"
      ],
      "metadata": {
        "id": "22UvDbHKdKVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"floor\"\n",
        "trans48_args_s = AttrDict()\n",
        "args_dict_48 = {\n",
        "    \"data_file_name\": \"pig_latin_small\", #pig_latin_small debug\n",
        "    \"cuda\": use_cuda,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 48,\n",
        "    \"num_transformer_layers\": 4,\n",
        "    \"loss_seq_span\": \"target_only\",\n",
        "    \"num_layers\": 4,\n",
        "    \"vocab_size\": 30 \n",
        "}\n",
        "trans48_args_s.update(args_dict_48)\n",
        "print_opts(trans48_args_s)\n",
        "trans48_decoder_s, trans48_losses_s, idx_dict = train(trans48_args_s)"
      ],
      "metadata": {
        "id": "EPn-U3JVC3SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"floor\"\n",
        "trans64_args_s = AttrDict()\n",
        "args_dict_64 = {\n",
        "    \"data_file_name\": \"pig_latin_small\", #pig_latin_small debug\n",
        "    \"cuda\": use_cuda,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 64,\n",
        "    \"num_transformer_layers\": 4,\n",
        "    \"loss_seq_span\": \"target_only\",\n",
        "    \"num_layers\": 4,\n",
        "    \"vocab_size\": 30 \n",
        "}\n",
        "trans64_args_s.update(args_dict_64)\n",
        "print_opts(trans64_args_s)\n",
        "trans64_decoder_s, trans64_losses_s, idx_dict = train(trans64_args_s)"
      ],
      "metadata": {
        "id": "ZZxBqkcVDCIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"floor\"\n",
        "trans96_args_s = AttrDict()\n",
        "args_dict_96 = {\n",
        "    \"data_file_name\": \"pig_latin_small\", #pig_latin_small debug\n",
        "    \"cuda\": use_cuda,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 96,\n",
        "    \"num_transformer_layers\": 4,\n",
        "    \"loss_seq_span\": \"target_only\",\n",
        "    \"num_layers\": 4,\n",
        "    \"vocab_size\": 30 \n",
        "}\n",
        "trans96_args_s.update(args_dict_96)\n",
        "print_opts(trans96_args_s)\n",
        "trans96_decoder_s, trans96_losses_s, idx_dict = train(trans96_args_s)"
      ],
      "metadata": {
        "id": "DPsVBnPFDTl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"floor\"\n",
        "trans128_args_s = AttrDict()\n",
        "args_dict_128 = {\n",
        "    \"data_file_name\": \"pig_latin_small\", #pig_latin_small debug\n",
        "    \"cuda\": use_cuda,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 128,\n",
        "    \"num_transformer_layers\": 4,\n",
        "    \"loss_seq_span\": \"target_only\",\n",
        "    \"num_layers\": 4,\n",
        "    \"vocab_size\": 30 \n",
        "}\n",
        "trans128_args_s.update(args_dict_128)\n",
        "print_opts(trans128_args_s)\n",
        "trans128_decoder_s, trans128_losses_s, idx_dict = train(trans128_args_s)"
      ],
      "metadata": {
        "id": "N-SngzYkDZmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getf_loss_flop(data, degree=5):\n",
        "    x, y = data[3], data[1]\n",
        "    p = np.polyfit(x, y, degree)\n",
        "    return p\n",
        "\n",
        "def plot_loss_flop_o(data_list, num_params, degree=5):\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    for data, params in zip(data_list, num_params):\n",
        "        x, y = data[3], data[1]\n",
        "        ax.plot(x, y, label=f'{params}')\n",
        "\n",
        "    ax.legend(title='# Params', loc = \"lower left\")\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('FLOPs')\n",
        "    plt.ylabel('Val Loss')\n",
        "    plt.title('Val Loss vs FLOPs') \n",
        "\n",
        "def plot_loss_flop(data_list, num_params, degree=5):\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    for data, params in zip(data_list, num_params):\n",
        "        x, y = data[3], data[1]\n",
        "        p = np.polyfit(x, y, degree)\n",
        "        x_min, x_max = min(x), max(x)\n",
        "        px = np.logspace(np.log10(x_min), np.log10(x_max), 100)\n",
        "        py = np.polyval(p, px)\n",
        "        ax.plot(px, py, label=f'{params}')\n",
        "\n",
        "    ax.legend(title='# Params', loc = \"lower left\")\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('FLOPs')\n",
        "    plt.ylabel('Val Loss')\n",
        "    plt.title('Val Loss (Poly Approx.) vs FLOPs')  "
      ],
      "metadata": {
        "id": "psuLTDsSW8X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_names = ['Train loss', 'Val loss', 'Tokens', 'Flops', 'Number of Examples', 'Number of Parameters']\n",
        "\n",
        "hidden_list = [16, 32, 48, 64, 96, 128]\n",
        "num_params = [transformer_params(30, hidden, 4) for hidden in hidden_list]\n",
        "data_list = [trans16_losses_s, trans32_losses_s, trans48_losses_s, trans64_losses_s, trans96_losses_s, trans128_losses_s]\n",
        "\n",
        "plot_loss_flop_o(data_list, num_params)\n",
        "plot_loss_flop(data_list, num_params)"
      ],
      "metadata": {
        "id": "fEzHD4rxYo8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: IsoFLOP profiles"
      ],
      "metadata": {
        "id": "x_oDmwk9HIC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scatter_data(data_list, num_params, f_list, tflops):\n",
        "    x, y = [], []\n",
        "    for i, f in enumerate(f_list):\n",
        "        if tflops <= max(data_list[i][3]) and tflops >= min(data_list[i][3]):\n",
        "            x.append(num_params[i])\n",
        "            y.append(np.polyval(f, tflops))\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def find_optimal_params(x, y):\n",
        "    # ------------\n",
        "    # FILL THIS IN\n",
        "    # ------------\n",
        "    # p = np.polyfit( , , 2)\n",
        "    # optimal_params =  \n",
        "    return p, optimal_params"
      ],
      "metadata": {
        "id": "IrHwe4-iYxEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_isoflop(data_list, num_params, degree=5):\n",
        "    colors = ['#E41A1C', '#377EB8','#4DAF4A','#984EA3','#FF7F00','#FFFF33','#A65628']\n",
        "    target_flops = [8e10, 16e10, 32e10, 64e10, 128e10]\n",
        "    f_list = [getf_loss_flop(data, degree=5) for data in data_list]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    for fidx, tflops in enumerate(target_flops):\n",
        "        x, y = get_scatter_data(data_list, num_params, f_list, tflops)\n",
        "        p, optimal_params = find_optimal_params(x, y)\n",
        "\n",
        "        ax.scatter(np.array(x), np.array(y), label=f'{tflops/1e12} TFlops', color=colors[fidx])\n",
        "        px = np.logspace(np.log10(x[0]), np.log10(x[-1]), 100)\n",
        "        py = np.polyval(p, np.log10(px))\n",
        "        ax.plot(px, py, color=colors[fidx])\n",
        "        ax.axvline(x=optimal_params, color=colors[fidx], linestyle='--')\n",
        "\n",
        "    ax.legend(loc = \"lower left\")\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('# Parameters')\n",
        "    plt.ylabel('Val Loss')\n",
        "    plt.title('Val Loss vs # Parameters')  "
      ],
      "metadata": {
        "id": "Z7VG-JIjIo8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_isoflop(data_list, num_params)"
      ],
      "metadata": {
        "id": "1uC-c74IiHTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Parameters vs FLOPs"
      ],
      "metadata": {
        "id": "eTO9I8MoHUh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_linear_log(x, y):\n",
        "    # ------------\n",
        "    # FILL THIS IN\n",
        "    # ------------\n",
        "    # m, c = np.polyfit(, , 1)\n",
        "    return m, c\n"
      ],
      "metadata": {
        "id": "EsjQiLoWuUkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_flops_params(data_list, num_params):\n",
        "    colors = ['#E41A1C', '#377EB8','#4DAF4A','#984EA3','#FF7F00','#FFFF33','#A65628']\n",
        "    target_flops = [8e10, 16e10, 32e10, 64e10, 128e10]\n",
        "    f_list = [getf_loss_flop(data, degree=5) for data in data_list]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    params = []\n",
        "    for fidx, tflops in enumerate(target_flops):\n",
        "        x, y = get_scatter_data(data_list, num_params, f_list, tflops)\n",
        "        p, optimal_params = find_optimal_params(x, y)\n",
        "        params.append(optimal_params)\n",
        "\n",
        "    ax.scatter(target_flops, params)\n",
        "    m, c = fit_linear_log(target_flops, params)\n",
        "    print(f\"y = {m}x + {c}\")\n",
        "\n",
        "    lx = np.logspace(np.log10(1e10), np.log10(1e15), 100)\n",
        "    ly = 10**(np.log10(lx) * m + c)\n",
        "    ax.plot(lx, ly, color = colors[0])\n",
        "\n",
        "    ax.legend(loc = \"lower left\")\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('FLOPs')\n",
        "    plt.ylabel('# Parameters')\n",
        "    plt.xlim()\n",
        "    plt.title('Compute Optimal Models') \n",
        "    plt.grid()"
      ],
      "metadata": {
        "id": "CJ2ciTkEIepL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_flops_params(data_list, num_params)"
      ],
      "metadata": {
        "id": "4JC1AZEnqRhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Tokens vs FLOPs"
      ],
      "metadata": {
        "id": "av84Y3u9HiE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import bisect\n",
        "\n",
        "def find_hidden(params, x_min=16, x_max=1e4, tol=1e-4):\n",
        "    f = lambda h: transformer_params(vocab_size=30, hidden_size=h, num_layers=4)\n",
        "    # Define the function to find the root of\n",
        "    def g(x):\n",
        "        return f(x) - params\n",
        "    \n",
        "    # Find the root of the function using the bisection method\n",
        "    x_root = bisect(g, x_min, x_max, rtol=tol)\n",
        "    return x_root\n",
        "\n",
        "def find_tokens(params, flops, seq_len):\n",
        "    hidden = find_hidden(params)\n",
        "    flops_per_example = transformer_flops(seq_len=seq_len, vocab_size=30, \n",
        "                                          hidden_size=hidden, num_layers=4)\n",
        "    return flops/flops_per_example * seq_len\n",
        "\n",
        "def plot_flops_tokens(data_list, num_params):\n",
        "    seq_len = 19.55\n",
        "    colors = ['#E41A1C', '#377EB8','#4DAF4A','#984EA3','#FF7F00','#FFFF33','#A65628']\n",
        "    target_flops = [8e10, 16e10, 32e10, 64e10, 128e10]\n",
        "    f_list = [getf_loss_flop(data, degree=5) for data in data_list]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "    params = []\n",
        "    tokens = []\n",
        "    for fidx, tflops in enumerate(target_flops):\n",
        "        x, y = get_scatter_data(data_list, num_params, f_list, tflops)\n",
        "        p, optimal_params = find_optimal_params(x, y)\n",
        "        params.append(optimal_params)\n",
        "        tokens.append(find_tokens(optimal_params, tflops, seq_len))\n",
        "\n",
        "    ax.scatter(target_flops, tokens)\n",
        "\n",
        "    m, c = fit_linear_log(target_flops, tokens)\n",
        "    print(f\"y = {m}x + {c}\")\n",
        "\n",
        "    lx = np.logspace(np.log10(1e10), np.log10(1e15), 100)\n",
        "    ly = 10**(np.log10(lx) * m + c)\n",
        "    ax.plot(lx, ly, color = colors[0])\n",
        "\n",
        "    ax.legend(loc = \"lower left\")\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('FLOPs')\n",
        "    plt.ylabel('# Tokens')\n",
        "    plt.title('Compute Optimal Models') \n",
        "    plt.grid()"
      ],
      "metadata": {
        "id": "TWjN16svIMI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_flops_tokens(data_list, num_params)"
      ],
      "metadata": {
        "id": "voexCYSuyKVh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "s9IS9B9-yUU5",
        "9DaTdRNuUra7",
        "4BIpGwANoQOg",
        "pbvpn4MaV0I1",
        "0yh08KhgnA30",
        "bRWfRdmVVjUl",
        "NPKHAiPQmOAi",
        "Pbp1XQCp882O",
        "rEt6SplHo7NV",
        "nv6WDG36iYmR",
        "N20B2EKigj5U",
        "av84Y3u9HiE5"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}