{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImLCXm8IsSS2"
      },
      "source": [
        "# Download the Cora data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xRN47p1SKRgP",
        "outputId": "1ccf4c89-2e12-4f4d-cae3-ecd75d7a01b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-29 08:24:07--  https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
            "Resolving linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)... 128.114.47.74\n",
            "Connecting to linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)|128.114.47.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168052 (164K) [application/x-gzip]\n",
            "Saving to: ‘cora.tgz’\n",
            "\n",
            "cora.tgz            100%[===================>] 164.11K   344KB/s    in 0.5s    \n",
            "\n",
            "2023-03-29 08:24:09 (344 KB/s) - ‘cora.tgz’ saved [168052/168052]\n",
            "\n",
            "cora/\n",
            "cora/README\n",
            "cora/cora.cites\n",
            "cora/cora.content\n"
          ]
        }
      ],
      "source": [
        "! wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
        "! tar -zxvf cora.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXIYzURA4OKg"
      },
      "source": [
        "# import modules and set random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uJQYMX02_z0M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "seed = 0\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgOv1h7YsK-5"
      },
      "source": [
        "# Loading and preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kXPHN61i9keB"
      },
      "outputs": [],
      "source": [
        "def encode_onehot(labels):\n",
        "    # The classes must be sorted before encoding to enable static class encoding.\n",
        "    # In other words, make sure the first class always maps to index 0.\n",
        "    classes = sorted(list(set(labels)))\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def load_data(path=\"/content/cora/\", dataset=\"cora\", training_samples=140):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "    adj = normalize_adj(adj)\n",
        "\n",
        "    # Random indexes\n",
        "    idx_rand = torch.randperm(len(labels))\n",
        "    # Nodes for training\n",
        "    idx_train = idx_rand[:training_samples]\n",
        "    # Nodes for validation\n",
        "    idx_val= idx_rand[training_samples:]\n",
        "\n",
        "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"symmetric normalization\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzCZVd1JsbHr"
      },
      "source": [
        "## check the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KlsKjMKx8_b7",
        "outputId": "ec9c766e-97aa-4353-9423-e346c59cfcaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "adj, features, labels, idx_train, idx_val = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mxrv21rLnpiZ",
        "outputId": "030039ca-4e76-4b02-e3fd-993572ea1e1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1667, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.2000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.2000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2500]])\n",
            "torch.Size([2708, 2708])\n"
          ]
        }
      ],
      "source": [
        "print(adj)\n",
        "print(adj.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lWrDf0iWnpqV",
        "outputId": "ed8d211c-49f9-496e-9b95-f102973004b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "torch.Size([2708, 1433])\n"
          ]
        }
      ],
      "source": [
        "print(features)\n",
        "print(features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TUkt2JJdsuA2",
        "outputId": "f3cb0ea3-8cbf-4865-d2c2-df243d06c875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 5, 4,  ..., 1, 0, 2])\n",
            "tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "2708\n"
          ]
        }
      ],
      "source": [
        "print(labels)\n",
        "print(labels.unique())\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iGP18jNAs1Gp",
        "outputId": "79db6656-b84a-478f-a670-2ae06765468c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140\n",
            "2568\n"
          ]
        }
      ],
      "source": [
        "print(len(idx_train))\n",
        "print(len(idx_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHqIcfH-vIic"
      },
      "source": [
        "# Vanilla GCN for node classification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Graph Convolution layer (Your Task)\n",
        "\n",
        "This module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n",
        "1.   perform initial transformation: $\\mathbf{s} = \\mathbf{W} \\times \\mathbf{h} ^{(l)}$\n",
        "2.   multiply $\\mathbf{s}$ by normalized adjacency matrix: $\\mathbf{h'} = \\mathbf{A} \\times \\mathbf{s}$"
      ],
      "metadata": {
        "id": "f48tylWyjLPE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "M-fU8L7f41VZ"
      },
      "outputs": [],
      "source": [
        "class GraphConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    A Graph Convolution Layer (GCN)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        \"\"\"\n",
        "        * `in_features`, $F$, is the number of input features per node\n",
        "        * `out_features`, $F'$, is the number of output features per node\n",
        "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
        "        \"\"\"\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        # TODO: initialize the weight W that maps the input feature (dim F ) to output feature (dim F')\n",
        "        # hint: use nn.Linear()\n",
        "        ############ Your code here ###################################\n",
        "\n",
        "        self.W = nn.Linear(in_features, out_features, bias=bias)\n",
        "\n",
        "        ###############################################################\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # TODO: transform input feature to output (don't forget to use the adjacency matrix \n",
        "        # to sum over neighbouring nodes )\n",
        "        # hint: use the linear layer you declared above. \n",
        "        # hint: you can use torch.spmm() sparse matrix multiplication to handle the \n",
        "        #       adjacency matrix\n",
        "        ############ Your code here ###################################\n",
        "        s = self.W(input)\n",
        "        output = torch.spmm(adj, s)\n",
        "        return output\n",
        "\n",
        "\n",
        "        ###############################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define GCN (Your Task)\n",
        "\n",
        "you will implement a two-layer GCN with ReLU activation function and Dropout after the first Conv layer."
      ],
      "metadata": {
        "id": "RxBELCxkjF6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "    '''\n",
        "    A two-layer GCN\n",
        "    '''\n",
        "    def __init__(self, nfeat, n_hidden, n_classes, dropout, bias=True):\n",
        "        \"\"\"\n",
        "        * `nfeat`, is the number of input features per node of the first layer\n",
        "        * `n_hidden`, number of hidden units\n",
        "        * `n_classes`, total number of classes for classification\n",
        "        * `dropout`, the dropout ratio\n",
        "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
        "        \"\"\"\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "        # TODO: Initialization\n",
        "        # (1) 2 GraphConvolution() layers. \n",
        "        # (2) 1 Dropout layer\n",
        "        # (3) 1 activation function: ReLU()\n",
        "        ############ Your code here ###################################\n",
        "        self.gc1 = GraphConvolution(nfeat, n_hidden, bias)\n",
        "        self.gc2 = GraphConvolution(n_hidden, n_classes, bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "\n",
        "        ###############################################################\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # TODO: the input will pass through the first graph convolution layer, \n",
        "        # the activation function, the dropout layer, then the second graph \n",
        "        # convolution layer. No activation function for the \n",
        "        # last layer. Return the logits. \n",
        "        ############ Your code here ###################################\n",
        "        x = self.gc1(x, adj)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.gc2(x, adj)\n",
        "        return x\n",
        "\n",
        "        ###############################################################"
      ],
      "metadata": {
        "id": "HtVr2cN8jD5t"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX1d9F1G508r"
      },
      "source": [
        "## define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HyhqJ39OCzNN"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXsdid6C5K1c"
      },
      "source": [
        "## training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "bjlYeoFPFAWm"
      },
      "outputs": [],
      "source": [
        "args = {\"training_samples\": 140,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 0.01,\n",
        "        \"weight_decay\": 5e-4,\n",
        "        \"hidden\": 16,\n",
        "        \"dropout\": 0.5,\n",
        "        \"bias\": True, \n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Qbx0uc-9G5vs"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = criterion(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "\n",
        "    loss_val = criterion(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = criterion(output[idx_val], labels[idx_val])\n",
        "    acc_test = accuracy(output[idx_val], labels[idx_val])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TjNiui83FYBr",
        "outputId": "1f59316c-ecee-46d5-a0fc-481ba2fa3ca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "model = GCN(nfeat=features.shape[1],\n",
        "            n_hidden=args[\"hidden\"],\n",
        "            n_classes=labels.max().item() + 1,\n",
        "            dropout=args[\"dropout\"]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
        "\n",
        "\n",
        "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
        "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training Vanilla GCN"
      ],
      "metadata": {
        "id": "1W6tqqj16iz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "WSjUYJPSlnOU",
        "outputId": "6ee41bca-e359-4d56-cc2d-2dd23d1f4341",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9470 acc_train: 0.0571 loss_val: 1.9317 acc_val: 0.2995 time: 0.1186s\n",
            "Epoch: 0002 loss_train: 1.9367 acc_train: 0.3071 loss_val: 1.9266 acc_val: 0.3006 time: 0.0035s\n",
            "Epoch: 0003 loss_train: 1.9300 acc_train: 0.3286 loss_val: 1.9205 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0004 loss_train: 1.9221 acc_train: 0.3286 loss_val: 1.9140 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0005 loss_train: 1.9120 acc_train: 0.3286 loss_val: 1.9069 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0006 loss_train: 1.9016 acc_train: 0.3286 loss_val: 1.8997 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0007 loss_train: 1.8923 acc_train: 0.3286 loss_val: 1.8925 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0008 loss_train: 1.8783 acc_train: 0.3286 loss_val: 1.8850 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0009 loss_train: 1.8668 acc_train: 0.3286 loss_val: 1.8770 acc_val: 0.3006 time: 0.0035s\n",
            "Epoch: 0010 loss_train: 1.8543 acc_train: 0.3286 loss_val: 1.8687 acc_val: 0.3006 time: 0.0037s\n",
            "Epoch: 0011 loss_train: 1.8420 acc_train: 0.3286 loss_val: 1.8605 acc_val: 0.3006 time: 0.0109s\n",
            "Epoch: 0012 loss_train: 1.8320 acc_train: 0.3286 loss_val: 1.8525 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0013 loss_train: 1.8131 acc_train: 0.3286 loss_val: 1.8447 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0014 loss_train: 1.8050 acc_train: 0.3286 loss_val: 1.8371 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0015 loss_train: 1.7879 acc_train: 0.3286 loss_val: 1.8298 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0016 loss_train: 1.7767 acc_train: 0.3286 loss_val: 1.8229 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0017 loss_train: 1.7765 acc_train: 0.3357 loss_val: 1.8166 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0018 loss_train: 1.7523 acc_train: 0.3286 loss_val: 1.8106 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0019 loss_train: 1.7364 acc_train: 0.3286 loss_val: 1.8050 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0020 loss_train: 1.7174 acc_train: 0.3357 loss_val: 1.7997 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0021 loss_train: 1.7084 acc_train: 0.3357 loss_val: 1.7947 acc_val: 0.3006 time: 0.0085s\n",
            "Epoch: 0022 loss_train: 1.7026 acc_train: 0.3286 loss_val: 1.7898 acc_val: 0.3006 time: 0.0116s\n",
            "Epoch: 0023 loss_train: 1.6837 acc_train: 0.3357 loss_val: 1.7849 acc_val: 0.3006 time: 0.0046s\n",
            "Epoch: 0024 loss_train: 1.6817 acc_train: 0.3357 loss_val: 1.7794 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0025 loss_train: 1.6661 acc_train: 0.3500 loss_val: 1.7734 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0026 loss_train: 1.6608 acc_train: 0.3357 loss_val: 1.7668 acc_val: 0.3006 time: 0.0033s\n",
            "Epoch: 0027 loss_train: 1.6621 acc_train: 0.3429 loss_val: 1.7594 acc_val: 0.3002 time: 0.0033s\n",
            "Epoch: 0028 loss_train: 1.6276 acc_train: 0.3429 loss_val: 1.7512 acc_val: 0.3002 time: 0.0033s\n",
            "Epoch: 0029 loss_train: 1.6297 acc_train: 0.3429 loss_val: 1.7424 acc_val: 0.3002 time: 0.0033s\n",
            "Epoch: 0030 loss_train: 1.6241 acc_train: 0.3429 loss_val: 1.7329 acc_val: 0.3018 time: 0.0033s\n",
            "Epoch: 0031 loss_train: 1.6143 acc_train: 0.3500 loss_val: 1.7229 acc_val: 0.3022 time: 0.0033s\n",
            "Epoch: 0032 loss_train: 1.5790 acc_train: 0.3714 loss_val: 1.7124 acc_val: 0.3026 time: 0.0033s\n",
            "Epoch: 0033 loss_train: 1.5539 acc_train: 0.3643 loss_val: 1.7019 acc_val: 0.3033 time: 0.0033s\n",
            "Epoch: 0034 loss_train: 1.5482 acc_train: 0.3714 loss_val: 1.6913 acc_val: 0.3053 time: 0.0033s\n",
            "Epoch: 0035 loss_train: 1.5431 acc_train: 0.4143 loss_val: 1.6805 acc_val: 0.3084 time: 0.0033s\n",
            "Epoch: 0036 loss_train: 1.5197 acc_train: 0.3857 loss_val: 1.6697 acc_val: 0.3143 time: 0.0033s\n",
            "Epoch: 0037 loss_train: 1.5006 acc_train: 0.4286 loss_val: 1.6589 acc_val: 0.3228 time: 0.0058s\n",
            "Epoch: 0038 loss_train: 1.4922 acc_train: 0.4571 loss_val: 1.6481 acc_val: 0.3326 time: 0.0033s\n",
            "Epoch: 0039 loss_train: 1.4848 acc_train: 0.4786 loss_val: 1.6370 acc_val: 0.3388 time: 0.0033s\n",
            "Epoch: 0040 loss_train: 1.4792 acc_train: 0.5286 loss_val: 1.6258 acc_val: 0.3489 time: 0.0033s\n",
            "Epoch: 0041 loss_train: 1.4564 acc_train: 0.5143 loss_val: 1.6148 acc_val: 0.3567 time: 0.0033s\n",
            "Epoch: 0042 loss_train: 1.4316 acc_train: 0.5714 loss_val: 1.6041 acc_val: 0.3629 time: 0.0033s\n",
            "Epoch: 0043 loss_train: 1.4223 acc_train: 0.5286 loss_val: 1.5931 acc_val: 0.3719 time: 0.0033s\n",
            "Epoch: 0044 loss_train: 1.3929 acc_train: 0.5643 loss_val: 1.5821 acc_val: 0.3847 time: 0.0033s\n",
            "Epoch: 0045 loss_train: 1.3900 acc_train: 0.5643 loss_val: 1.5712 acc_val: 0.3991 time: 0.0033s\n",
            "Epoch: 0046 loss_train: 1.3533 acc_train: 0.6429 loss_val: 1.5601 acc_val: 0.4124 time: 0.0033s\n",
            "Epoch: 0047 loss_train: 1.3412 acc_train: 0.6000 loss_val: 1.5489 acc_val: 0.4295 time: 0.0033s\n",
            "Epoch: 0048 loss_train: 1.3149 acc_train: 0.6357 loss_val: 1.5376 acc_val: 0.4431 time: 0.0033s\n",
            "Epoch: 0049 loss_train: 1.3273 acc_train: 0.6500 loss_val: 1.5260 acc_val: 0.4622 time: 0.0033s\n",
            "Epoch: 0050 loss_train: 1.2981 acc_train: 0.6571 loss_val: 1.5142 acc_val: 0.4751 time: 0.0033s\n",
            "Epoch: 0051 loss_train: 1.2603 acc_train: 0.6214 loss_val: 1.5021 acc_val: 0.4879 time: 0.0033s\n",
            "Epoch: 0052 loss_train: 1.2756 acc_train: 0.6500 loss_val: 1.4898 acc_val: 0.5062 time: 0.0033s\n",
            "Epoch: 0053 loss_train: 1.2867 acc_train: 0.6500 loss_val: 1.4772 acc_val: 0.5280 time: 0.0067s\n",
            "Epoch: 0054 loss_train: 1.2133 acc_train: 0.6643 loss_val: 1.4648 acc_val: 0.5510 time: 0.0035s\n",
            "Epoch: 0055 loss_train: 1.2249 acc_train: 0.6714 loss_val: 1.4528 acc_val: 0.5685 time: 0.0033s\n",
            "Epoch: 0056 loss_train: 1.2195 acc_train: 0.7143 loss_val: 1.4414 acc_val: 0.5798 time: 0.0081s\n",
            "Epoch: 0057 loss_train: 1.1783 acc_train: 0.7286 loss_val: 1.4301 acc_val: 0.5868 time: 0.0071s\n",
            "Epoch: 0058 loss_train: 1.1717 acc_train: 0.7071 loss_val: 1.4186 acc_val: 0.5931 time: 0.0038s\n",
            "Epoch: 0059 loss_train: 1.1588 acc_train: 0.7143 loss_val: 1.4071 acc_val: 0.5950 time: 0.0067s\n",
            "Epoch: 0060 loss_train: 1.1371 acc_train: 0.7071 loss_val: 1.3956 acc_val: 0.5989 time: 0.0053s\n",
            "Epoch: 0061 loss_train: 1.1069 acc_train: 0.7429 loss_val: 1.3842 acc_val: 0.6020 time: 0.0029s\n",
            "Epoch: 0062 loss_train: 1.1061 acc_train: 0.7714 loss_val: 1.3727 acc_val: 0.6102 time: 0.0029s\n",
            "Epoch: 0063 loss_train: 1.0989 acc_train: 0.7071 loss_val: 1.3608 acc_val: 0.6172 time: 0.0028s\n",
            "Epoch: 0064 loss_train: 1.1122 acc_train: 0.7357 loss_val: 1.3491 acc_val: 0.6192 time: 0.0028s\n",
            "Epoch: 0065 loss_train: 1.0888 acc_train: 0.7571 loss_val: 1.3378 acc_val: 0.6269 time: 0.0036s\n",
            "Epoch: 0066 loss_train: 1.0626 acc_train: 0.7571 loss_val: 1.3269 acc_val: 0.6328 time: 0.0107s\n",
            "Epoch: 0067 loss_train: 1.0532 acc_train: 0.7500 loss_val: 1.3164 acc_val: 0.6382 time: 0.0039s\n",
            "Epoch: 0068 loss_train: 1.0178 acc_train: 0.7714 loss_val: 1.3059 acc_val: 0.6429 time: 0.0031s\n",
            "Epoch: 0069 loss_train: 1.0048 acc_train: 0.7571 loss_val: 1.2957 acc_val: 0.6507 time: 0.0029s\n",
            "Epoch: 0070 loss_train: 1.0034 acc_train: 0.7929 loss_val: 1.2858 acc_val: 0.6585 time: 0.0029s\n",
            "Epoch: 0071 loss_train: 1.0271 acc_train: 0.7571 loss_val: 1.2761 acc_val: 0.6608 time: 0.0029s\n",
            "Epoch: 0072 loss_train: 1.0108 acc_train: 0.7643 loss_val: 1.2663 acc_val: 0.6636 time: 0.0028s\n",
            "Epoch: 0073 loss_train: 0.9846 acc_train: 0.8071 loss_val: 1.2563 acc_val: 0.6671 time: 0.0028s\n",
            "Epoch: 0074 loss_train: 0.9583 acc_train: 0.7714 loss_val: 1.2464 acc_val: 0.6698 time: 0.0028s\n",
            "Epoch: 0075 loss_train: 0.9124 acc_train: 0.7929 loss_val: 1.2367 acc_val: 0.6717 time: 0.0029s\n",
            "Epoch: 0076 loss_train: 0.9403 acc_train: 0.8214 loss_val: 1.2272 acc_val: 0.6776 time: 0.0029s\n",
            "Epoch: 0077 loss_train: 0.8896 acc_train: 0.8286 loss_val: 1.2181 acc_val: 0.6846 time: 0.0031s\n",
            "Epoch: 0078 loss_train: 0.9118 acc_train: 0.8000 loss_val: 1.2091 acc_val: 0.6885 time: 0.0030s\n",
            "Epoch: 0079 loss_train: 0.8532 acc_train: 0.8143 loss_val: 1.2001 acc_val: 0.6912 time: 0.0027s\n",
            "Epoch: 0080 loss_train: 0.8924 acc_train: 0.8000 loss_val: 1.1910 acc_val: 0.6924 time: 0.0029s\n",
            "Epoch: 0081 loss_train: 0.8939 acc_train: 0.7786 loss_val: 1.1818 acc_val: 0.6982 time: 0.0028s\n",
            "Epoch: 0082 loss_train: 0.8478 acc_train: 0.8357 loss_val: 1.1729 acc_val: 0.6990 time: 0.0028s\n",
            "Epoch: 0083 loss_train: 0.8602 acc_train: 0.8214 loss_val: 1.1643 acc_val: 0.7005 time: 0.0028s\n",
            "Epoch: 0084 loss_train: 0.8472 acc_train: 0.7857 loss_val: 1.1553 acc_val: 0.7044 time: 0.0028s\n",
            "Epoch: 0085 loss_train: 0.8299 acc_train: 0.8357 loss_val: 1.1466 acc_val: 0.7076 time: 0.0029s\n",
            "Epoch: 0086 loss_train: 0.8456 acc_train: 0.7857 loss_val: 1.1380 acc_val: 0.7083 time: 0.0027s\n",
            "Epoch: 0087 loss_train: 0.8306 acc_train: 0.8214 loss_val: 1.1298 acc_val: 0.7044 time: 0.0029s\n",
            "Epoch: 0088 loss_train: 0.8371 acc_train: 0.8286 loss_val: 1.1221 acc_val: 0.7029 time: 0.0028s\n",
            "Epoch: 0089 loss_train: 0.8023 acc_train: 0.8286 loss_val: 1.1148 acc_val: 0.7048 time: 0.0027s\n",
            "Epoch: 0090 loss_train: 0.8061 acc_train: 0.8571 loss_val: 1.1080 acc_val: 0.7068 time: 0.0027s\n",
            "Epoch: 0091 loss_train: 0.7966 acc_train: 0.8286 loss_val: 1.1014 acc_val: 0.7111 time: 0.0108s\n",
            "Epoch: 0092 loss_train: 0.8029 acc_train: 0.8000 loss_val: 1.0951 acc_val: 0.7153 time: 0.0027s\n",
            "Epoch: 0093 loss_train: 0.7566 acc_train: 0.8286 loss_val: 1.0886 acc_val: 0.7185 time: 0.0040s\n",
            "Epoch: 0094 loss_train: 0.7884 acc_train: 0.8286 loss_val: 1.0819 acc_val: 0.7169 time: 0.0074s\n",
            "Epoch: 0095 loss_train: 0.7043 acc_train: 0.8500 loss_val: 1.0746 acc_val: 0.7188 time: 0.0027s\n",
            "Epoch: 0096 loss_train: 0.7659 acc_train: 0.8000 loss_val: 1.0672 acc_val: 0.7212 time: 0.0028s\n",
            "Epoch: 0097 loss_train: 0.7581 acc_train: 0.8214 loss_val: 1.0603 acc_val: 0.7235 time: 0.0027s\n",
            "Epoch: 0098 loss_train: 0.7025 acc_train: 0.8429 loss_val: 1.0536 acc_val: 0.7305 time: 0.0027s\n",
            "Epoch: 0099 loss_train: 0.7414 acc_train: 0.8571 loss_val: 1.0466 acc_val: 0.7309 time: 0.0030s\n",
            "Epoch: 0100 loss_train: 0.7454 acc_train: 0.8357 loss_val: 1.0397 acc_val: 0.7278 time: 0.0028s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 0.6641s\n",
            "Test set results: loss= 1.0397 accuracy= 0.7278\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(args[\"epochs\"]):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# evaluating\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF3eM6DhHfE_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCFwzVLmPXnH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Attention Networks"
      ],
      "metadata": {
        "id": "mKHEyXp1EVdo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx15HdotKnt_"
      },
      "source": [
        "## Graph attention layer (Your task)\n",
        "A GAT is made up of multiple such layers. In this section, you will implement a single graph attention layer. Similar to the `GraphConvolution()`, this `GraphAttentionLayer()` module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$. However, instead of weighing each neighbouring node based on the adjacency matrix, we will use self attention to learn the relative importance of each neighbouring node. Recall from HW4 where you are asked to write out the equation for single headed attention, here we will implement multi-headed attention, which involves the following steps: \n",
        "\n",
        "\n",
        "### The initial transformation\n",
        "In GCN above, you have completed similar transformation. But here, we need to define a weight matrix and perform this transformation for each head: $\\overrightarrow{s^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$. We will perform a single linear transformation and then split it up for each head later. Note the input $\\overrightarrow{h}$ has shape `[n_nodes, in_features]` and $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads * n_hidden]`. Remember to reshape $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads, n_hidden]` for later uses. Note: set `bias=False` for this linear transformation. \n",
        "\n",
        "### attention score\n",
        "We calculate these for each head $k$. Here for simplicity of the notation, we omit $k$ in the following equations. The attention scores are defined as the follows: \n",
        "$$e_{ij} = a(\\mathbf{W} \\overrightarrow{h_i}, \\mathbf{W} \\overrightarrow{h_j}) =a(\\overrightarrow{s_i}, \\overrightarrow{s_j})$$, \n",
        "where $e_{ij}$ is the attention score (importance) of node $j$ to node $i$.\n",
        "We will have to calculate this for each head. $a$ is the attention mechanism, that calculates the attention score. The paper concatenates $\\overrightarrow{s_i}$, $\\overrightarrow{s_j}$ and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{2 F'}$ followed by a $\\text{LeakyReLU}$. $$e_{ij} = \\text{LeakyReLU} \\Big(\n",
        "\\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$$\n",
        "\n",
        "#### How to vectorize this? Some hints: \n",
        "1. `tensor.repeat()` gives you $\\{\\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, ...\\}$.\n",
        "\n",
        "2. `tensor.repeat_interleave()` gives you\n",
        "$\\{\\overrightarrow{s_1}, \\overrightarrow{s_1}, \\dots, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_2}, ...\\}$.\n",
        "\n",
        "3. concatenate to get $\\Big[\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j} \\Big]$ for all pairs of $i, j$. Reshape $\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}$ has shape of `[n_nodes, n_nodes, n_heads, 2 * n_hidden]`\n",
        "\n",
        "4. apply the attention layer and non-linear activation function to get $e_{ij} = \\text{LeakyReLU} \\Big( \\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$, where $\\mathbf{a}^\\top$ is a single linear transformation that maps from dimension `n_hidden * 2` to `1`. Note: set the `bias=False` for this linear transformation. $\\mathbf{e}$ is of shape `[n_nodes, n_nodes, n_heads, 1]`. Remove the last dimension `1` using `squeeze()`. \n",
        "\n",
        "\n",
        "#### Perform softmax \n",
        "First, we need to mask $e_{ij}$ based on adjacency matrix. We only need to sum over the neighbouring nodes for the attention calculation. Set the elements in $e_{ij}$ to $- \\infty$ if there is no edge from $i$ to $j$ for the softmax calculation. We need to do this for all heads and the adjacency matrix is the same for each head. Use `tensor.masked_fill()` to mask $e_{ij}$ based on adjacency matrix for all heads. Hint: reshape the adjacency matrix to `[n_nodes, n_nodes, 1]` using `unsqueeze()`. \n",
        "Now we are ready to normalize attention scores (or coefficients) $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =  \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}$$\n",
        "\n",
        "#### Apply dropout\n",
        "Apply the dropout layer. (this step is easy)\n",
        "\n",
        "#### Calculate final output for each head\n",
        "$$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{s^k_j}$$\n",
        "\n",
        "\n",
        "#### Concat or Mean\n",
        "Finally we concateneate the transformed features: $\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$. In the code, we only need to reshape the tensor to shape of `[n_nodes, n_heads * n_hidden]`. Note that if it is the final layer, then it doesn't make sense to do concatenation anymore. Instead, we sum over the `n_heads` dimension: $\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "wVu7rcOuAUZz"
      },
      "outputs": [],
      "source": [
        "class GraphAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
        "                 is_concat: bool = True,\n",
        "                 dropout: float = 0.6,\n",
        "                 alpha: float = 0.2):\n",
        "        \"\"\"\n",
        "        in_features: F, the number of input features per node\n",
        "        out_features: F', the number of output features per node\n",
        "        n_heads: K, the number of attention heads\n",
        "        is_concat: whether the multi-head results should be concatenated or averaged\n",
        "        dropout: the dropout probability\n",
        "        alpha: the negative slope for leaky relu activation\n",
        "        \"\"\"\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "\n",
        "        self.is_concat = is_concat\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        if is_concat:\n",
        "            assert out_features % n_heads == 0\n",
        "            self.n_hidden = out_features // n_heads\n",
        "        else:\n",
        "            self.n_hidden = out_features\n",
        "\n",
        "        # TODO: initialize the following modules: \n",
        "        # (1) self.W: Linear layer that transform the input feature before self attention. \n",
        "        # You should NOT use for loops for the multiheaded implementation (set bias = Flase)\n",
        "        # (2) self.attention: Linear layer that compute the attention score (set bias = Flase)\n",
        "        # (3) self.activation: Activation function (LeakyReLU whith negative_slope=alpha)\n",
        "        # (4) self.softmax: Softmax function (what's the dim to compute the summation?)\n",
        "        # (5) self.dropout_layer: Dropout function(with ratio=dropout)\n",
        "        ################ your code here ########################\n",
        "        \n",
        "        self.W = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
        "        self.attention = nn.Linear(self.n_hidden * 2, 1, bias=False)\n",
        "        self.activation = nn.LeakyReLU(negative_slope=alpha)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "        ########################################################\n",
        "\n",
        "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
        "        # Number of nodes\n",
        "        n_nodes = h.shape[0]\n",
        "        \n",
        "        # TODO: \n",
        "        # (1) calculate s = Wh and reshape it to [n_nodes, n_heads, n_hidden] \n",
        "        #     (you can use tensor.view() function)\n",
        "        # (2) get [s_i || s_j] using tensor.repeat(), repeat_interleave(), torch.cat(), tensor.view()  \n",
        "        # (3) apply the attention layer \n",
        "        # (4) apply the activation layer (you will get the attention score e)\n",
        "        # (5) remove the last dimension 1 use tensor.squeeze()\n",
        "        # (6) mask the attention score with the adjacency matrix (if there's no edge, assign it to -inf)\n",
        "        #     note: check the dimensions of e and your adjacency matrix. You may need to use the function unsqueeze()\n",
        "        # (7) apply softmax \n",
        "        # (8) apply dropout_layer \n",
        "        ############## Your code here #########################################\n",
        "\n",
        "        s = self.W(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
        "        s_j = s.repeat(n_nodes,1,1)\n",
        "        s_i = s.repeat_interleave(n_nodes, dim=0)\n",
        "        s_concat = torch.cat([s_i, s_j], dim=-1)\n",
        "        s_concat = s_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
        "\n",
        "        e = self.activation(self.attention(s_concat))\n",
        "        e = e.squeeze(-1)\n",
        "        e = e.masked_fill(adj_mat.unsqueeze(-1) == 0, float('-inf'))\n",
        "        a = self.softmax(e)\n",
        "        a = self.dropout_layer(a)\n",
        "\n",
        "        #######################################################################\n",
        "\n",
        "        # Summation \n",
        "        h_prime = torch.einsum('ijh,jhf->ihf', a, s) #[n_nodes, n_heads, n_hidden]\n",
        "\n",
        "\n",
        "        # TODO: Concat or Mean\n",
        "        # Concatenate the heads\n",
        "        if self.is_concat:\n",
        "            ############## Your code here #########################################\n",
        "\n",
        "            return h_prime.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
        "\n",
        "            #######################################################################\n",
        "        # Take the mean of the heads (for the last layer)\n",
        "        else:\n",
        "            ############## Your code here #########################################\n",
        "\n",
        "            return h_prime.mean(dim=1)\n",
        "\n",
        "            #######################################################################\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define GAT network\n",
        "it's really similar to how we defined GCN. We followed the paper to use two attention layers and ELU() activation function. "
      ],
      "metadata": {
        "id": "YOSk_ZShi2nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "\n",
        "    def __init__(self, nfeat: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float, alpha: float):\n",
        "        \"\"\"\n",
        "        in_features: the number of features per node\n",
        "        n_hidden: the number of features in the first graph attention layer\n",
        "        n_classes: the number of classes\n",
        "        n_heads: the number of heads in the graph attention layers\n",
        "        dropout: the dropout probability\n",
        "        alpha: the negative input slope for leaky ReLU of the attention layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # First graph attention layer where we concatenate the heads\n",
        "        self.gc1 = GraphAttentionLayer(nfeat, n_hidden, n_heads, is_concat=True, dropout=dropout, alpha=alpha)\n",
        "        self.gc2 = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout, alpha=alpha)\n",
        "        self.activation = nn.ELU()  \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n",
        "        \"\"\"\n",
        "        x: the features vectors\n",
        "        adj_mat: the adjacency matrix\n",
        "        \"\"\"\n",
        "        x = self.dropout(x)\n",
        "        x = self.gc1(x, adj_mat)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.gc2(x, adj_mat)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jKNbUtPVi1Vs"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training GAT"
      ],
      "metadata": {
        "id": "CtRQ3Ced7RAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\"training_samples\": 140,\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 0.01,\n",
        "        \"weight_decay\": 5e-4,\n",
        "        \"hidden\": 16,\n",
        "        \"dropout\": 0.5,\n",
        "        \"bias\": True, \n",
        "        \"alpha\": 0.2,\n",
        "        \"n_heads\": 8\n",
        "        }"
      ],
      "metadata": {
        "id": "b7D5mYXC6zTG"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7MYaK98hDy7u",
        "outputId": "47b92634-1fe2-4c34-be6b-9fa7db955a2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ],
      "source": [
        "model = GAT(nfeat=features.shape[1],\n",
        "            n_hidden=args[\"hidden\"],\n",
        "            n_classes=labels.max().item() + 1,\n",
        "            dropout=args[\"dropout\"],\n",
        "            alpha=args[\"alpha\"],\n",
        "            n_heads=args[\"n_heads\"]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
        "\n",
        "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
        "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "E9FcfXwMDzEt",
        "outputId": "a354e56b-a366-469a-8a97-bf418cbcda82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9459 acc_train: 0.0929 loss_val: 1.9426 acc_val: 0.5424 time: 0.2785s\n",
            "Epoch: 0002 loss_train: 1.9405 acc_train: 0.4286 loss_val: 1.9386 acc_val: 0.4798 time: 0.2274s\n",
            "Epoch: 0003 loss_train: 1.9363 acc_train: 0.4786 loss_val: 1.9344 acc_val: 0.4178 time: 0.2281s\n",
            "Epoch: 0004 loss_train: 1.9262 acc_train: 0.5429 loss_val: 1.9297 acc_val: 0.3824 time: 0.2300s\n",
            "Epoch: 0005 loss_train: 1.9223 acc_train: 0.4643 loss_val: 1.9246 acc_val: 0.3711 time: 0.2428s\n",
            "Epoch: 0006 loss_train: 1.9159 acc_train: 0.5214 loss_val: 1.9193 acc_val: 0.3618 time: 0.2379s\n",
            "Epoch: 0007 loss_train: 1.9087 acc_train: 0.4643 loss_val: 1.9136 acc_val: 0.3551 time: 0.2281s\n",
            "Epoch: 0008 loss_train: 1.8973 acc_train: 0.4571 loss_val: 1.9076 acc_val: 0.3512 time: 0.2286s\n",
            "Epoch: 0009 loss_train: 1.8906 acc_train: 0.4571 loss_val: 1.9012 acc_val: 0.3474 time: 0.2306s\n",
            "Epoch: 0010 loss_train: 1.8766 acc_train: 0.4643 loss_val: 1.8945 acc_val: 0.3415 time: 0.2384s\n",
            "Epoch: 0011 loss_train: 1.8811 acc_train: 0.4714 loss_val: 1.8875 acc_val: 0.3384 time: 0.2281s\n",
            "Epoch: 0012 loss_train: 1.8519 acc_train: 0.4500 loss_val: 1.8802 acc_val: 0.3361 time: 0.2283s\n",
            "Epoch: 0013 loss_train: 1.8566 acc_train: 0.4286 loss_val: 1.8725 acc_val: 0.3337 time: 0.2324s\n",
            "Epoch: 0014 loss_train: 1.8357 acc_train: 0.4429 loss_val: 1.8645 acc_val: 0.3322 time: 0.2332s\n",
            "Epoch: 0015 loss_train: 1.8399 acc_train: 0.4000 loss_val: 1.8563 acc_val: 0.3306 time: 0.2334s\n",
            "Epoch: 0016 loss_train: 1.8219 acc_train: 0.4143 loss_val: 1.8477 acc_val: 0.3290 time: 0.2294s\n",
            "Epoch: 0017 loss_train: 1.8020 acc_train: 0.4286 loss_val: 1.8389 acc_val: 0.3275 time: 0.2296s\n",
            "Epoch: 0018 loss_train: 1.7861 acc_train: 0.4143 loss_val: 1.8298 acc_val: 0.3271 time: 0.2294s\n",
            "Epoch: 0019 loss_train: 1.7631 acc_train: 0.4571 loss_val: 1.8204 acc_val: 0.3267 time: 0.2316s\n",
            "Epoch: 0020 loss_train: 1.7985 acc_train: 0.3714 loss_val: 1.8110 acc_val: 0.3271 time: 0.2289s\n",
            "Epoch: 0021 loss_train: 1.7157 acc_train: 0.4500 loss_val: 1.8012 acc_val: 0.3259 time: 0.2288s\n",
            "Epoch: 0022 loss_train: 1.7470 acc_train: 0.3929 loss_val: 1.7912 acc_val: 0.3255 time: 0.2286s\n",
            "Epoch: 0023 loss_train: 1.7355 acc_train: 0.3857 loss_val: 1.7813 acc_val: 0.3244 time: 0.2292s\n",
            "Epoch: 0024 loss_train: 1.7144 acc_train: 0.4071 loss_val: 1.7712 acc_val: 0.3244 time: 0.2300s\n",
            "Epoch: 0025 loss_train: 1.7047 acc_train: 0.3929 loss_val: 1.7610 acc_val: 0.3240 time: 0.2295s\n",
            "Epoch: 0026 loss_train: 1.6991 acc_train: 0.4071 loss_val: 1.7509 acc_val: 0.3228 time: 0.2298s\n",
            "Epoch: 0027 loss_train: 1.7123 acc_train: 0.3714 loss_val: 1.7409 acc_val: 0.3224 time: 0.2301s\n",
            "Epoch: 0028 loss_train: 1.6396 acc_train: 0.4214 loss_val: 1.7307 acc_val: 0.3240 time: 0.2332s\n",
            "Epoch: 0029 loss_train: 1.6830 acc_train: 0.3857 loss_val: 1.7208 acc_val: 0.3252 time: 0.2298s\n",
            "Epoch: 0030 loss_train: 1.6348 acc_train: 0.4000 loss_val: 1.7107 acc_val: 0.3259 time: 0.2291s\n",
            "Epoch: 0031 loss_train: 1.6109 acc_train: 0.4214 loss_val: 1.7005 acc_val: 0.3271 time: 0.2296s\n",
            "Epoch: 0032 loss_train: 1.5775 acc_train: 0.3929 loss_val: 1.6904 acc_val: 0.3298 time: 0.2300s\n",
            "Epoch: 0033 loss_train: 1.5771 acc_train: 0.4286 loss_val: 1.6802 acc_val: 0.3314 time: 0.2298s\n",
            "Epoch: 0034 loss_train: 1.5787 acc_train: 0.4214 loss_val: 1.6699 acc_val: 0.3361 time: 0.2304s\n",
            "Epoch: 0035 loss_train: 1.5578 acc_train: 0.4571 loss_val: 1.6595 acc_val: 0.3407 time: 0.2324s\n",
            "Epoch: 0036 loss_train: 1.5033 acc_train: 0.4000 loss_val: 1.6491 acc_val: 0.3438 time: 0.2304s\n",
            "Epoch: 0037 loss_train: 1.4824 acc_train: 0.4714 loss_val: 1.6385 acc_val: 0.3485 time: 0.2298s\n",
            "Epoch: 0038 loss_train: 1.5293 acc_train: 0.4500 loss_val: 1.6279 acc_val: 0.3528 time: 0.2301s\n",
            "Epoch: 0039 loss_train: 1.5715 acc_train: 0.4143 loss_val: 1.6175 acc_val: 0.3602 time: 0.2296s\n",
            "Epoch: 0040 loss_train: 1.4959 acc_train: 0.4500 loss_val: 1.6070 acc_val: 0.3688 time: 0.2303s\n",
            "Epoch: 0041 loss_train: 1.5391 acc_train: 0.4429 loss_val: 1.5966 acc_val: 0.3785 time: 0.2298s\n",
            "Epoch: 0042 loss_train: 1.4107 acc_train: 0.5143 loss_val: 1.5860 acc_val: 0.3879 time: 0.2301s\n",
            "Epoch: 0043 loss_train: 1.4736 acc_train: 0.5071 loss_val: 1.5756 acc_val: 0.3988 time: 0.2304s\n",
            "Epoch: 0044 loss_train: 1.4586 acc_train: 0.5000 loss_val: 1.5652 acc_val: 0.4100 time: 0.2308s\n",
            "Epoch: 0045 loss_train: 1.4578 acc_train: 0.5286 loss_val: 1.5548 acc_val: 0.4237 time: 0.2297s\n",
            "Epoch: 0046 loss_train: 1.3591 acc_train: 0.5643 loss_val: 1.5444 acc_val: 0.4369 time: 0.2310s\n",
            "Epoch: 0047 loss_train: 1.4232 acc_train: 0.5214 loss_val: 1.5338 acc_val: 0.4564 time: 0.2310s\n",
            "Epoch: 0048 loss_train: 1.3965 acc_train: 0.5143 loss_val: 1.5233 acc_val: 0.4790 time: 0.2317s\n",
            "Epoch: 0049 loss_train: 1.3680 acc_train: 0.5786 loss_val: 1.5127 acc_val: 0.4984 time: 0.2303s\n",
            "Epoch: 0050 loss_train: 1.3879 acc_train: 0.5929 loss_val: 1.5021 acc_val: 0.5156 time: 0.2309s\n",
            "Epoch: 0051 loss_train: 1.2887 acc_train: 0.6214 loss_val: 1.4915 acc_val: 0.5296 time: 0.2306s\n",
            "Epoch: 0052 loss_train: 1.3314 acc_train: 0.6143 loss_val: 1.4809 acc_val: 0.5518 time: 0.2316s\n",
            "Epoch: 0053 loss_train: 1.3880 acc_train: 0.5929 loss_val: 1.4704 acc_val: 0.5635 time: 0.2307s\n",
            "Epoch: 0054 loss_train: 1.3570 acc_train: 0.6286 loss_val: 1.4599 acc_val: 0.5736 time: 0.2312s\n",
            "Epoch: 0055 loss_train: 1.3515 acc_train: 0.6143 loss_val: 1.4495 acc_val: 0.5841 time: 0.2301s\n",
            "Epoch: 0056 loss_train: 1.3116 acc_train: 0.7000 loss_val: 1.4391 acc_val: 0.5981 time: 0.2309s\n",
            "Epoch: 0057 loss_train: 1.2663 acc_train: 0.6357 loss_val: 1.4286 acc_val: 0.6114 time: 0.2298s\n",
            "Epoch: 0058 loss_train: 1.3455 acc_train: 0.7000 loss_val: 1.4183 acc_val: 0.6211 time: 0.2308s\n",
            "Epoch: 0059 loss_train: 1.3109 acc_train: 0.6500 loss_val: 1.4082 acc_val: 0.6324 time: 0.2301s\n",
            "Epoch: 0060 loss_train: 1.2298 acc_train: 0.6929 loss_val: 1.3981 acc_val: 0.6406 time: 0.2302s\n",
            "Epoch: 0061 loss_train: 1.2250 acc_train: 0.7214 loss_val: 1.3879 acc_val: 0.6480 time: 0.2303s\n",
            "Epoch: 0062 loss_train: 1.1913 acc_train: 0.7286 loss_val: 1.3776 acc_val: 0.6554 time: 0.2301s\n",
            "Epoch: 0063 loss_train: 1.2897 acc_train: 0.6857 loss_val: 1.3674 acc_val: 0.6616 time: 0.2305s\n",
            "Epoch: 0064 loss_train: 1.2592 acc_train: 0.6929 loss_val: 1.3571 acc_val: 0.6639 time: 0.2308s\n",
            "Epoch: 0065 loss_train: 1.1327 acc_train: 0.7643 loss_val: 1.3468 acc_val: 0.6698 time: 0.2306s\n",
            "Epoch: 0066 loss_train: 1.1966 acc_train: 0.7214 loss_val: 1.3363 acc_val: 0.6760 time: 0.2306s\n",
            "Epoch: 0067 loss_train: 1.1443 acc_train: 0.7429 loss_val: 1.3259 acc_val: 0.6787 time: 0.2304s\n",
            "Epoch: 0068 loss_train: 1.1706 acc_train: 0.7429 loss_val: 1.3156 acc_val: 0.6822 time: 0.2304s\n",
            "Epoch: 0069 loss_train: 1.1859 acc_train: 0.7500 loss_val: 1.3054 acc_val: 0.6846 time: 0.2302s\n",
            "Epoch: 0070 loss_train: 1.1939 acc_train: 0.7714 loss_val: 1.2955 acc_val: 0.6877 time: 0.2305s\n",
            "Epoch: 0071 loss_train: 1.1741 acc_train: 0.7143 loss_val: 1.2858 acc_val: 0.6904 time: 0.2306s\n",
            "Epoch: 0072 loss_train: 1.1678 acc_train: 0.7357 loss_val: 1.2763 acc_val: 0.6928 time: 0.2307s\n",
            "Epoch: 0073 loss_train: 1.1886 acc_train: 0.7286 loss_val: 1.2670 acc_val: 0.6967 time: 0.2301s\n",
            "Epoch: 0074 loss_train: 1.1778 acc_train: 0.7714 loss_val: 1.2579 acc_val: 0.6998 time: 0.2324s\n",
            "Epoch: 0075 loss_train: 1.1378 acc_train: 0.7571 loss_val: 1.2492 acc_val: 0.7033 time: 0.2308s\n",
            "Epoch: 0076 loss_train: 1.0574 acc_train: 0.7857 loss_val: 1.2406 acc_val: 0.7052 time: 0.2305s\n",
            "Epoch: 0077 loss_train: 1.0788 acc_train: 0.7786 loss_val: 1.2321 acc_val: 0.7068 time: 0.2310s\n",
            "Epoch: 0078 loss_train: 1.0792 acc_train: 0.7500 loss_val: 1.2236 acc_val: 0.7091 time: 0.2300s\n",
            "Epoch: 0079 loss_train: 1.1240 acc_train: 0.7429 loss_val: 1.2152 acc_val: 0.7087 time: 0.2309s\n",
            "Epoch: 0080 loss_train: 1.1209 acc_train: 0.7571 loss_val: 1.2069 acc_val: 0.7114 time: 0.2313s\n",
            "Epoch: 0081 loss_train: 1.1182 acc_train: 0.7357 loss_val: 1.1989 acc_val: 0.7130 time: 0.2309s\n",
            "Epoch: 0082 loss_train: 1.0032 acc_train: 0.7714 loss_val: 1.1909 acc_val: 0.7134 time: 0.2303s\n",
            "Epoch: 0083 loss_train: 1.1308 acc_train: 0.6929 loss_val: 1.1832 acc_val: 0.7134 time: 0.2310s\n",
            "Epoch: 0084 loss_train: 1.0348 acc_train: 0.7929 loss_val: 1.1757 acc_val: 0.7130 time: 0.2314s\n",
            "Epoch: 0085 loss_train: 1.0263 acc_train: 0.7857 loss_val: 1.1681 acc_val: 0.7157 time: 0.2301s\n",
            "Epoch: 0086 loss_train: 0.9695 acc_train: 0.7929 loss_val: 1.1606 acc_val: 0.7177 time: 0.2302s\n",
            "Epoch: 0087 loss_train: 1.1271 acc_train: 0.7429 loss_val: 1.1532 acc_val: 0.7196 time: 0.2306s\n",
            "Epoch: 0088 loss_train: 1.0251 acc_train: 0.7714 loss_val: 1.1460 acc_val: 0.7216 time: 0.2307s\n",
            "Epoch: 0089 loss_train: 1.0518 acc_train: 0.7643 loss_val: 1.1389 acc_val: 0.7227 time: 0.2303s\n",
            "Epoch: 0090 loss_train: 0.9502 acc_train: 0.7714 loss_val: 1.1318 acc_val: 0.7259 time: 0.2302s\n",
            "Epoch: 0091 loss_train: 0.9912 acc_train: 0.8286 loss_val: 1.1247 acc_val: 0.7270 time: 0.2305s\n",
            "Epoch: 0092 loss_train: 0.9945 acc_train: 0.7429 loss_val: 1.1177 acc_val: 0.7278 time: 0.2302s\n",
            "Epoch: 0093 loss_train: 0.9651 acc_train: 0.7357 loss_val: 1.1111 acc_val: 0.7298 time: 0.2302s\n",
            "Epoch: 0094 loss_train: 1.0585 acc_train: 0.7357 loss_val: 1.1048 acc_val: 0.7313 time: 0.2301s\n",
            "Epoch: 0095 loss_train: 0.9640 acc_train: 0.7857 loss_val: 1.0984 acc_val: 0.7336 time: 0.2304s\n",
            "Epoch: 0096 loss_train: 1.0024 acc_train: 0.7929 loss_val: 1.0922 acc_val: 0.7348 time: 0.2301s\n",
            "Epoch: 0097 loss_train: 0.9965 acc_train: 0.7500 loss_val: 1.0861 acc_val: 0.7368 time: 0.2290s\n",
            "Epoch: 0098 loss_train: 0.9836 acc_train: 0.8214 loss_val: 1.0800 acc_val: 0.7379 time: 0.2302s\n",
            "Epoch: 0099 loss_train: 0.9000 acc_train: 0.7857 loss_val: 1.0739 acc_val: 0.7379 time: 0.2299s\n",
            "Epoch: 0100 loss_train: 0.9587 acc_train: 0.7714 loss_val: 1.0680 acc_val: 0.7383 time: 0.2325s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 23.3709s\n",
            "Test set results: loss= 1.0680 accuracy= 0.7383\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(args[\"epochs\"]):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Testing\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question: (Your task)\n",
        "Compare the evaluation results for Vanilla GCN and GAT. Comment on the discrepancy in their performance (if any) and briefly explain why you think it's the case (in 1-2 sentences). "
      ],
      "metadata": {
        "id": "n6Ox3fbTG7rc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urJ8Q-neDzHU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZhHh8k4DzJu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmvJ46OfGlf2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "a4_GCN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}