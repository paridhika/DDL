{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1cOtLH2E3ll"
      },
      "source": [
        "# CSC413 Tutorial 2: Autograd & PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x2gZAiSE3lo"
      },
      "source": [
        "## Tutorial overview\n",
        "\n",
        "- Part 1: Interactive notebooks\n",
        "\n",
        "- Part 2: Autograd and PyTorch\n",
        "\n",
        "- Part 3: PyTorch models (with increasing levels of complexity and abstraction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0V9uW9mE3lp"
      },
      "source": [
        "##  Part 1: Interactive notebooks\n",
        "\n",
        "### Overview\n",
        "\n",
        "- Keep documentation, code, and outputs in the same file.\n",
        "\n",
        "- Many different implementations (e.g. Jupyter, Google Colab, Zeppelin, etc.).\n",
        "\n",
        "- Many different languages (e.g. Python, C++ (xeus), Scala, etc.).\n",
        "\n",
        "- Execute your code block-by-block: easy to prototype and debug.\n",
        "\n",
        "- Widely used in data science projects.\n",
        "\n",
        "### Google Colab\n",
        "\n",
        "- Hosted interactive notebooks. \n",
        "\n",
        "- Pros:\n",
        "\n",
        "  - Easy to use and share. \n",
        "  \n",
        "  - Can get a free GPU or TPU (make sure to select a runtime with a GPU).\n",
        "\n",
        "- You can complete all the programming assignments on Colab.\n",
        "\n",
        "### Jupyter notebooks / lab\n",
        "\n",
        "- Run on your own server (GPU/CPU).\n",
        "\n",
        "- Pros:\n",
        "\n",
        "  - Local.\n",
        "  \n",
        "  - Customizable interface (e.g., Shortcuts, Plugins, Templates).\n",
        "\n",
        "  - Easy to call your own library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW6E9VvcmYpd"
      },
      "source": [
        "## Part 2.1: Automatic differentiation\n",
        "\n",
        "- In class, we have seen how the backpropagation algorithm could be used to\n",
        "compute gradients for basically any neural net architecture, as long as all the individual pieces of the computation are differentiable.\n",
        "\n",
        "- However, implementing backprop manually is like writing assembly language:\n",
        "you need to apply chain rule to each node in your computation graph. \n",
        "\n",
        "- You may notice that the entire procedure is mechanical. Can we write a program to build the computation graph and apply the backprop updates? Yes, it is exactly the autodiff engine does.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusing concepts\n",
        "\n",
        "- **Backpropagation**: the mathematical algorithm we use to compute the gradient.\n",
        "\n",
        "- **Automatic differentiation** (AutoDiff): any software that implements backpropagation.\n",
        "  - Examples: **Autograd**, TensorFlow, PyTorch, Jax, etc."
      ],
      "metadata": {
        "id": "G4hx7YavysiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approaches for Computing Derivatives\n",
        "\n",
        "- To understand why we really need AutoDiff, we may want to contrast it with several different approaches for computing derivatives.\n",
        "\n",
        "- **Symbolic differentiation:** manipulate the mathematical expressions to get derivatives.\n",
        "    - Takes a math expression and returns a math expression: $f(x) = x^2 \\rightarrow \\frac{df(x)}{dx} = 2x$.\n",
        "    - Used in Mathematica, Maple, SymPy, etc.\n",
        "\n",
        "- **Numeric differentiation:** Approximating derivatives by finite differences:\n",
        "$$\n",
        "\\frac{\\partial}{\\partial x_i} f(x_1, \\dots, x_N) = \\lim_{h \\to 0} \\frac{f(x_1, \\dots, x_i + h, \\dots, x_N) - f(x_1, \\dots, x_i - h, \\dots, x_N)}{2h}\n",
        "$$\n",
        "\n",
        "- **Automatic differentiation:** Write a program that efficiently computes the derivatives.\n",
        "    - Reverse Mode AD: A method to get exact derivatives efficiently, by storing information as you go forward that you can reuse as you go backwards\n",
        "    - This is efficient for graphs with large fan-in, like most loss functions in ML. In machine learning, we have functions that have large fan-in, e.g. a neural net can have millions of parameters, that all squeeze down to one scalar that tells you how well it predicts something."
      ],
      "metadata": {
        "id": "by7nKutvyhY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General idea for Implementation\n",
        "* Create a \"tape\" data structure that tracks the operations performed in computing a function.\n",
        "* Overload primitives to:\n",
        "    - Add themselves to the tape when called.\n",
        "    - Compute gradients with respect to their local inputs.\n",
        "* _Forward pass_ computes the function, and adds operations to the tape.\n",
        "* _Backward pass_ accumulates the local gradients using the chain rule."
      ],
      "metadata": {
        "id": "llvmUF-pyhce"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JxunitHmYpf"
      },
      "source": [
        "### Autograd\n",
        "\n",
        "* [Autograd](https://github.com/HIPS/autograd) is a Python package for automatic differentiation.\n",
        "* To install Autograd:\n",
        "                pip install autograd\n",
        "* There are a lot of great [examples](https://github.com/HIPS/autograd/tree/master/examples) provided with the source code.\n",
        "\n",
        "### What can Autograd do?\n",
        "\n",
        "From the Autograd Github repository:\n",
        "\n",
        "* Autograd can automatically differentiate native Python and Numpy code.\n",
        "* It can handle a large subset of Python's features, including loops, conditional statements (if/else), recursion and closures.\n",
        "* It can also compute higher-order derivatives.\n",
        "* It uses reverse-mode differentiation (a.k.a. backpropagation) so it can efficiently take gradients of scalar-valued functions with respect to array-valued arguments.\n",
        "* It constructs a computation graph _implicitly_, by tracking the sequence of operations that have been performed during the execution of a program.\n",
        "\n",
        "### Learn more about Automatic differentiation:\n",
        "\n",
        "- Roger's Notes on Automatic Differentiation.\n",
        "- Ryan Adams' talk: https://www.youtube.com/watch?v=sq2gPzlrM0g\n",
        "- Backpropagation notes from Stanford's CS231n: http://cs231n.github.io/optimization-2/\n",
        "- Autograd Github Repository (contains a tutorial and examples): https://github.com/HIPS/autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kDjpkTqmYph"
      },
      "source": [
        "### Autograd basic usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F3euyjuE3ls"
      },
      "outputs": [],
      "source": [
        "import autograd.numpy as jnp  # Import thinly-wrapped numpy\n",
        "from autograd import grad  # Basicallly the only autograd function you need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szDksM6wmYpn"
      },
      "outputs": [],
      "source": [
        "# Define a function like normal, using Python and (autograd's) NumPy\n",
        "def tanh(x):\n",
        "    y = jnp.exp(-x)\n",
        "    return (1.0 - y) / (1.0 + y)\n",
        "\n",
        "\n",
        "# Create a *function* that computes the gradient of tanh\n",
        "grad_tanh = grad(tanh)\n",
        "\n",
        "# Evaluate the gradient at x = 1.0\n",
        "print(grad_tanh(1.0))\n",
        "\n",
        "# Compare to numeric gradient computed using finite differences\n",
        "print((tanh(1.0001) - tanh(0.9999)) / 0.0002)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDGKdlknJMuK"
      },
      "source": [
        "## Part 2.2: PyTorch\n",
        "\n",
        "- Makes it possible to work with arrays and tensors efficiently in Python (wraps NumPy for CPU tensors).\n",
        "\n",
        "- Adds GPU support.\n",
        "\n",
        "- Adds automatic differentiation.\n",
        "\n",
        "- Provides a high-level abstractions for working with neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHx0nbsrE3lv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import torch\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYkp3o3zE3lw"
      },
      "source": [
        "### PyTorch â€” API\n",
        "\n",
        "See: https://pytorch.org/docs/stable/index.html (especially `torch`, `torch.nn`, `torch.nn.functional`, and `torch.Tensor`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JrqCxaFE3lw"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
        "\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReldVOHaE3lw"
      },
      "outputs": [],
      "source": [
        "x_np = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n",
        "x = torch.from_numpy(x_np)\n",
        "\n",
        "# Torch abstracts over NumPy but uses a NumPy-compatible representation under the hood\n",
        "x_np[0] = 100.0\n",
        "x[1] = 200.0\n",
        "x.data.numpy()[2] = 300\n",
        "\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "A8ntNuxDEwNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "WTTvOw9sENQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.reshape(-1, 1).shape"
      ],
      "metadata": {
        "id": "qzDfcieJE1pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr-RVmWlE3lx"
      },
      "outputs": [],
      "source": [
        "# Broadcasting\n",
        "x.reshape(-1, 1) + x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67T-w0KkE3lx"
      },
      "outputs": [],
      "source": [
        "# Dot product\n",
        "x @ x.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co-aXekIE3ly"
      },
      "source": [
        "### PyTorch â€” GPU support\n",
        "\n",
        "We can move PyTorch tensors to the GPU, which allows us to perform some computations much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo0IHCd0E3ly"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS4O12elE3ly"
      },
      "outputs": [],
      "source": [
        "x = x.to(device=device)\n",
        "\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa2rZfoNE3lz"
      },
      "outputs": [],
      "source": [
        "x @ x.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxSzk_bWE3lz"
      },
      "source": [
        "### PyTorch â€” Automatic differentiation\n",
        "\n",
        "PyTorch allows us to dynamically define computational graphs that can be evaluated efficently on GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v19hIquE3lz"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor([1.0])\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgI2RgjlE3lz"
      },
      "outputs": [],
      "source": [
        "param = torch.tensor([1.0], requires_grad=True)\n",
        "param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jOTeqZAE3l0"
      },
      "outputs": [],
      "source": [
        "(data + param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHorbqkE3l0"
      },
      "source": [
        "For a more concrete example, let's work with the function:\n",
        "\n",
        "$$f(x) = x^2 + 2x + 6$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKC3Lzwqs07E"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    return x ** 2 + 2 * x + 6\n",
        "\n",
        "\n",
        "np_x = np.array([4.0])\n",
        "x = torch.from_numpy(np_x).requires_grad_(True)\n",
        "y = f(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWQJmBlytIOV"
      },
      "outputs": [],
      "source": [
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCdN3mtXtX-N"
      },
      "outputs": [],
      "source": [
        "np_x = np.array([5.0])\n",
        "x = torch.from_numpy(np_x).requires_grad_(True)\n",
        "y = f(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TWXc5gmtdrA"
      },
      "outputs": [],
      "source": [
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v60TtmvumYps"
      },
      "source": [
        "### PyTorch autodiff vs. manual gradients via staged computation\n",
        "\n",
        "In this example, we will see how a complicated computation can be written as a composition of simpler functions, and how autodiff provides a scalable strategy for computing gradients using the chain rule.\n",
        "\n",
        "Say we want to write a function to compute the gradient of the *sigmoid function*:\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "We can write $\\sigma(x)$ as a composition of several elementary functions, as $\\sigma(x) = s(c(b(a(x))))$, where:\n",
        "\n",
        "$$\n",
        "a(x) = -x\n",
        "$$\n",
        "\n",
        "$$\n",
        "b(a) = e^a\n",
        "$$\n",
        "\n",
        "$$\n",
        "c(b) = 1 + b\n",
        "$$\n",
        "\n",
        "$$\n",
        "s(c) = \\frac{1}{c}\n",
        "$$\n",
        "\n",
        "Here, we have \"staged\" the computation such that it contains several intermediate variables, each of which are basic expressions that we can easily compute the local gradients.\n",
        "\n",
        "The computation graph for this expression is shown in the figure below. \n",
        " \n",
        "![Gradient Computation Image](https://drive.google.com/uc?export=view&id=1bvdPv0MI2eM3GeobsHFsFjLrLsibuhJa)\n",
        "\n",
        "The input to this function is $x$, and the output is represented by node $s$. We wish compute the gradient of $s$ with respect to $x$, $\\frac{\\partial s}{\\partial x}$. In order to make use of our intermediate computations, we can use the chain rule as follows:\n",
        "$$\n",
        "\\frac{\\partial s}{\\partial x} = \\frac{\\partial s}{\\partial c} \\frac{\\partial c}{\\partial b} \\frac{\\partial b}{\\partial a} \\frac{\\partial a}{\\partial x}\n",
        "$$\n",
        "\n",
        "<!--\n",
        "Given a vector-to-scalar function, $\\mathbb{R}^D \\to \\mathbb{R}$, composed of a set of primitive functions\n",
        "$\\mathbb{R}^M \\to \\mathbb{R}^N$ (for various $M$, $N$), the gradient of the composition is given by the product of the gradients of the primitive functions, according to the chain rule. But the chain rule doesnâ€™t prescribe the order in which to multiply the gradients. From the perspective of computational complexity, the order makes all the\n",
        "difference.\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5RUMyRsmYpt"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid function reimplemented for clarity. Use `torch.sigmoid` in real life!\"\"\"\n",
        "    y = 1.0 / (1.0 + torch.exp(-x))\n",
        "    return y\n",
        "\n",
        "\n",
        "def grad_sigmoid_pytorch(x):\n",
        "    x = x.clone().requires_grad_(True)\n",
        "    y = sigmoid(x)\n",
        "    y.backward()\n",
        "    return x.grad\n",
        "\n",
        "\n",
        "def grad_sigmoid_manual(x):\n",
        "    \"\"\"Implements the gradient of the logistic sigmoid function\n",
        "    $\\sigma(x) = 1 / (1 + e^{-x})$ using staged computation\n",
        "    \"\"\"\n",
        "    # Forward pass, keeping track of intermediate values for use in the\n",
        "    # backward pass\n",
        "    a = -x  # -x in denominator\n",
        "    b = np.exp(a)  # e^{-x} in denominator\n",
        "    c = 1 + b  # 1 + e^{-x} in denominator\n",
        "    s = 1.0 / c  # Final result, 1.0 / (1 + e^{-x})\n",
        "\n",
        "    # Backward pass\n",
        "    dsdc = -1.0 / (c ** 2)\n",
        "    dsdb = dsdc * 1\n",
        "    dsda = dsdb * torch.exp(a)\n",
        "    dsdx = dsda * (-1)\n",
        "\n",
        "    return dsdx\n",
        "\n",
        "\n",
        "def grad_sigmoid_symbolic(x):\n",
        "    # Since d sigmoid(x) / dx = sigmoid(x) * (1 - sigmoid(x))\n",
        "    s = sigmoid(x)\n",
        "    dsdx = s * (1 - s)\n",
        "    return dsdx\n",
        "\n",
        "\n",
        "input_x = torch.tensor([2.0])\n",
        "\n",
        "\n",
        "# Compare the results of manual and automatic gradient functions:\n",
        "print(\"autograd:\", grad_sigmoid_pytorch(input_x).item())\n",
        "print(\"manual:\", grad_sigmoid_manual(input_x).item())\n",
        "print(\"symbolic:\", grad_sigmoid_symbolic(input_x).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTHQYM2ImYq4"
      },
      "source": [
        "### Implementing custom gradients\n",
        "\n",
        "One thing you can do is define custom gradients for your own functions. There are several reasons you might want to do this, including:\n",
        "\n",
        "1. **Speed:** You may know a faster way to compute the gradient for a specific function.\n",
        "2. **Numerical Stability**.\n",
        "3. When your code depends on **external library calls**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUC_rGWwE3l2"
      },
      "outputs": [],
      "source": [
        "class MySigmoid(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    We can implement our own custom autograd Functions by subclassing\n",
        "    torch.autograd.Function and implementing the forward and backward passes\n",
        "    which operate on Tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we receive a Tensor containing the input and return\n",
        "        a Tensor containing the output. ctx is a context object that can be used\n",
        "        to stash information for backward computation. You can cache arbitrary\n",
        "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        ans = 1.0 / (1.0 + torch.exp(-input))\n",
        "        ctx.save_for_backward(input, ans)\n",
        "        return ans\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        \"\"\"\n",
        "        input, ans = ctx.saved_tensors\n",
        "        return grad_output * ans * (1 - ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydX0HQOJE3l2"
      },
      "outputs": [],
      "source": [
        "my_sigmoid = MySigmoid.apply\n",
        "\n",
        "x = input_x.clone().requires_grad_(True)\n",
        "y = my_sigmoid(x)\n",
        "y.backward()\n",
        "x.grad.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-ePOja_mYrA"
      },
      "source": [
        "## Part 3.1 Basic models\n",
        "\n",
        "The next three sections of the notebook show examples of using pytorch in the context of three problems:\n",
        "\n",
        "1. **1-D linear regression**, where we try to fit a model to a function $y = wx + b$\n",
        "2. **Linear regression using a polynomial feature map**, to fit a function of the form $y = w_0 + w_1 x + w_2 x^2 + \\dots + w_M x^M$\n",
        "3. **Nonlinear regression using a neural network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBs8UkXfmYrC"
      },
      "source": [
        "### Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLHB3U0BmYrD"
      },
      "source": [
        "#### Review\n",
        "\n",
        "We are given a set of data points $\\{ (x_1, t_1), (x_2, t_2), \\dots, (x_N, t_N) \\}$, where each point $(x_i, t_i)$ consists of an *input value* $x_i$ and a *target value* $t_i$.\n",
        "\n",
        "The **model** we use is:\n",
        "$$\n",
        "y_i = wx_i + b\n",
        "$$\n",
        "\n",
        "We want each predicted value $y_i$ to be close to the ground truth value $t_i$. In linear regression, we use squared error to quantify the disagreement between $y_i$ and $t_i$. The **loss function** for a single example is:\n",
        "$$\n",
        "L(y_i,t_i) = \\frac{1}{2} (y_i - t_i)^2\n",
        "$$\n",
        "\n",
        "The **cost function** is the loss averaged over all the training examples:\n",
        "$$\n",
        "E(w,b) = \\frac{1}{N} \\sum_{i=1}^N L(y_i, t_i) = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{2} \\left(wx_i + b - t_i \\right)^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-BhEYZOmYrJ"
      },
      "source": [
        "#### Data generation\n",
        "\n",
        "We generate a synthetic dataset $\\{ (x_i, t_i) \\}$ by first taking the $x_i$ to be linearly spaced in the range $[0, 10]$ and generating the corresponding value of $t_i$ using the following equation (where $w = 4$ and $b=10$):\n",
        "$$\n",
        "t_i = 4 x_i + 10 + \\epsilon\n",
        "$$\n",
        "\n",
        "Here, $\\epsilon \\sim N(0, 2)$ (that is, $\\epsilon$ is drawn from a Gaussian distribution with mean 0 and variance 2). This introduces some random fluctuation in the data, to mimic real data that has an underlying regularity, but for which individual observations are corrupted by random noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOLDubBYmYrK"
      },
      "outputs": [],
      "source": [
        "# In our synthetic data, we have w = 4 and b = 10\n",
        "N = 100  # Number of training data points\n",
        "x = np.linspace(0, 10, N)\n",
        "\n",
        "t = 4 * x + 10 + npr.normal(0, 2, x.shape[0])\n",
        "plt.plot(x, t, \"r.\")\n",
        "\n",
        "x = torch.from_numpy(x)\n",
        "t = torch.from_numpy(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWKVaOrimYrO"
      },
      "outputs": [],
      "source": [
        "# Initialize random parameters\n",
        "params = {\n",
        "    \"w\": torch.randn(1).requires_grad_(True),\n",
        "    \"b\": torch.randn(1).requires_grad_(True),\n",
        "}\n",
        "\n",
        "\n",
        "def cost(params):\n",
        "    y = params[\"w\"] * x + params[\"b\"]\n",
        "    return (1 / N) * torch.sum(0.5 * (y - t) ** 2)\n",
        "\n",
        "\n",
        "# Find the gradient of the cost function using pytorch\n",
        "num_epochs = 1000  # Number of epochs of training\n",
        "alpha = 0.01  # Learning rate\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    # Evaluate the gradient of the current parameters stored in params\n",
        "    loss = cost(params)\n",
        "    loss.backward()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"i: {i:<5d} loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Update parameters w and b\n",
        "    with torch.no_grad():\n",
        "        params[\"w\"].data = params[\"w\"] - alpha * params[\"w\"].grad\n",
        "        params[\"b\"].data = params[\"b\"] - alpha * params[\"b\"].grad\n",
        "        params[\"w\"].grad.zero_()\n",
        "        params[\"b\"].grad.zero_()\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6JKtls8mYrY"
      },
      "outputs": [],
      "source": [
        "# Plot the training data again, together with the line defined by y = wx + b\n",
        "# where w and b are our final learned parameters\n",
        "plt.plot(x, t, \"r.\")\n",
        "plt.plot([0, 10], [params[\"b\"].detach().numpy(), params[\"w\"].detach().numpy() * 10 + params[\"b\"].detach().numpy()], \"b-\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMf3GbOhmYrc"
      },
      "source": [
        "### Linear regression with a feature mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6sur0MDmYrd"
      },
      "source": [
        "In this example, we will fit a polynomial using linear regression with a polynomial feature mapping.\n",
        "The target function is:\n",
        "\n",
        "$$\n",
        "t = x^4 - 10 x^2 + 10 x + \\epsilon\n",
        "$$\n",
        "\n",
        "where $\\epsilon \\sim N(0, 4)$. \n",
        "\n",
        "This is an example of a _generalized linear model_, in which we perform a fixed nonlinear transformation of the inputs $\\mathbf{x} = (x_1, x_2, \\dots, x_D)$, and the model is still linear in the _parameters_. We can define a set of _feature mappings_ (also called feature functions or basis functions) $\\phi$ to implement the fixed transformations.\n",
        "\n",
        "In this case, we have $x \\in \\mathbb{R}$, and we define the feature mapping:\n",
        "$$\n",
        "\\mathbf{\\phi}(x) = \\begin{pmatrix}\\phi_1(x) \\\\ \\phi_2(x) \\\\ \\phi_3(x) \\\\ \\phi_4(x) \\\\ \\phi_5(x) \\end{pmatrix} = \\begin{pmatrix}1\\\\x\\\\x^2\\\\x^3\\\\x^4\\end{pmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-XKvKC4mYre"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic data\n",
        "N = 100  # Number of data points\n",
        "x = np.linspace(-3, 3, N)  # Generate N values linearly-spaced between -3 and 3\n",
        "t = x ** 4 - 10 * x ** 2 + 10 * x + npr.normal(0, 4, x.shape[0])  # Generate corresponding targets\n",
        "plt.plot(x, t, \"r.\")  # Plot data points\n",
        "\n",
        "t = torch.from_numpy(t).view(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGgROsxlmYrk"
      },
      "outputs": [],
      "source": [
        "M = 4  # Degree of polynomial to fit to the data (this is a hyperparameter)\n",
        "\n",
        "feature_matrix = torch.tensor(\n",
        "    [[item ** i for i in range(M + 1)] for item in x], dtype=torch.float32\n",
        ")\n",
        "\n",
        "params = {\n",
        "    \"w\": torch.randn(M + 1, 1).requires_grad_(True),\n",
        "}\n",
        "print(params[\"w\"].shape)\n",
        "\n",
        "\n",
        "def cost(params):\n",
        "    y = torch.mm(feature_matrix, params[\"w\"])\n",
        "    return (1.0 / N) * torch.sum(0.5 * (y - t) ** 2)\n",
        "\n",
        "\n",
        "# Compute the gradient of the cost function using Autograd\n",
        "\n",
        "num_epochs = 1000\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Manually implement gradient descent\n",
        "for i in range(num_epochs):\n",
        "    loss = cost(params)\n",
        "    loss.backward()\n",
        "    if i % 100 == 0:\n",
        "        print(f\"i: {i:<5d} loss: {loss.item():.4f}\")\n",
        "    with torch.no_grad():\n",
        "        params[\"w\"].data = params[\"w\"] - learning_rate * params[\"w\"].grad\n",
        "        params[\"w\"].grad.zero_()\n",
        "\n",
        "\n",
        "# Print the final learned parameters.\n",
        "print(params[\"w\"])\n",
        "w = params[\"w\"].detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY3XajWcmYrp"
      },
      "outputs": [],
      "source": [
        "# Plot the original training data again, together with the polynomial we fit\n",
        "plt.plot(x, t, \"r.\")\n",
        "plt.plot(x, np.dot(feature_matrix, w), \"b-\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWYEe11YmYrs"
      },
      "source": [
        "### Neural net regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7bnv5LemYru"
      },
      "source": [
        "In this example, we will implement a (nonlinear) regression model using a neural network. To implement and train a neural net using Autograd, you only have to define the forward pass of the network and the loss function you wish to use; you do _not_ need to implement the _backward pass_ of the network. When you take the gradient of the loss function using `grad`, Autograd automatically computes computes the backward pass. It essentially executes the backpropagation algorithm implicitly.\n",
        "\n",
        "![Neural Network Architecture for Regression](https://drive.google.com/uc?export=view&id=1iBNS40V_afm_Y1MUosDqeio0wbxgycfh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TuIWDkCmYr5"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic data\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "t = x ** 3 - 20 * x + 10 + npr.normal(0, 4, x.shape[0])\n",
        "plt.plot(x, t, \"r.\")\n",
        "\n",
        "x = torch.from_numpy(x).float()\n",
        "t = torch.from_numpy(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yphlCjMmmYr9"
      },
      "outputs": [],
      "source": [
        "inputs = x.reshape(x.shape[-1], 1)\n",
        "\n",
        "params = {\n",
        "    \"W1\": torch.randn(1, 4).requires_grad_(True),\n",
        "    \"b1\": torch.randn(4).requires_grad_(True),\n",
        "    \"W2\": torch.randn(4, 4).requires_grad_(True),\n",
        "    \"b2\": torch.randn(4).requires_grad_(True),\n",
        "    \"W3\": torch.randn(4, 1).requires_grad_(True),\n",
        "    \"b3\": torch.randn(1).requires_grad_(True),\n",
        "}\n",
        "\n",
        "\n",
        "# We can define an optimizer which takes care of updating parameters based on their gradient. We can use more complex optimizers like SGD+Momntum or Adam.\n",
        "optimizer = torch.optim.SGD(params.values(), lr=0.0001, weight_decay=0.0001, momentum=0.9)\n",
        "\n",
        "# Pytorch also has implementation of wide range of activation functions such as: Tanh, ReLU, LeakyReLU, ...\n",
        "nonlinearity = torch.nn.ReLU()\n",
        "\n",
        "\n",
        "def predict(params, inputs):\n",
        "    h1 = nonlinearity(torch.mm(inputs, params[\"W1\"]) + params[\"b1\"])\n",
        "    h2 = nonlinearity(torch.mm(h1, params[\"W2\"]) + params[\"b2\"])\n",
        "    output = torch.mm(h2, params[\"W3\"]) + params[\"b3\"]\n",
        "    return output\n",
        "\n",
        "\n",
        "def cost(params):\n",
        "    output = predict(params, inputs)\n",
        "    return (1.0 / inputs.shape[0]) * torch.sum(0.5 * (output.reshape(output.shape[0]) - t) ** 2)\n",
        "\n",
        "\n",
        "print(cost(params))\n",
        "\n",
        "num_epochs = 10000\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    # Evaluate the gradient of the current parameters stored in params\n",
        "    loss = cost(params)\n",
        "    if i % 500 == 0:\n",
        "        print(f\"i: {i:<5d} loss: {loss.item():.4f}\")\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "    final_y = predict(params, inputs)\n",
        "    plt.plot(x, t, \"r.\")\n",
        "    plt.plot(x, final_y, \"b-\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWU6AagIE3l6"
      },
      "source": [
        "## Part 3.2 Neural network models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkdDq61Ea38-"
      },
      "source": [
        "### MNIST classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiJiOPA5a-lu"
      },
      "source": [
        "[MNIST](http://yann.lecun.com/exdb/mnist/) is a famous dataset containing hand-written digits. The training set contains 60k and the test set contains 10k images. PyTorch has built-in functions for downloading well-known datasets like MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeeIOIMha5da"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "mnist_train = datasets.MNIST(\"data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "mnist_test = datasets.MNIST(\"../data\", train=False, download=True, transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTdP7vjzb35H"
      },
      "outputs": [],
      "source": [
        "print(mnist_train)\n",
        "print(mnist_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dWTw-Rgbh5O"
      },
      "outputs": [],
      "source": [
        "i = npr.randint(1, 50000)\n",
        "example = mnist_train[i]\n",
        "print(\"Label: \", example[1])\n",
        "plt.imshow(example[0].reshape((28, 28)), cmap=plt.cm.gray)\n",
        "plt.grid(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cYaF7PcB7v"
      },
      "source": [
        "Pytorch's DataLoader is responsible for creating an iterator over the dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WDyu63wcV3Z"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "mnist_train = datasets.MNIST(\"data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "mnist_test = datasets.MNIST(\"data\", train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "bs = 32\n",
        "train_dl = DataLoader(mnist_train, batch_size=bs)\n",
        "test_dl = DataLoader(mnist_test, batch_size=100)\n",
        "\n",
        "dataiter = iter(train_dl)\n",
        "images, labels = next(dataiter)\n",
        "viz = torchvision.utils.make_grid(images, nrow=10, padding=2).numpy()\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.imshow(np.transpose(viz, (1, 2, 0)))\n",
        "# ax.grid(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "779GU7DKdVbn"
      },
      "source": [
        "Using PyTorch's built-in functions, we can easily define any model like multi-layer perceptrons. After training, we just care about the average test accuracy, so let's write a function to compute the accuracy over the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Led8bErdZrw"
      },
      "outputs": [],
      "source": [
        "def get_test_stat(model, dl, device):\n",
        "    model.eval()\n",
        "    cum_loss, cum_acc = 0.0, 0.0\n",
        "    for i, (xb, yb) in enumerate(dl):\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        xb = xb.view(xb.size(0), -1)\n",
        "        y_pred = model(xb)\n",
        "        loss = loss_fn(y_pred, yb)\n",
        "        acc = (torch.max(y_pred.data, 1)[1] == yb).sum()  # accuracy(y_pred, yb)\n",
        "        cum_loss += loss.item() * len(yb)\n",
        "        cum_acc += acc.item() * len(yb)\n",
        "    cum_loss /= 10000\n",
        "    cum_acc /= 10000\n",
        "    model.train()\n",
        "    return cum_loss, cum_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohxuCpR9eLGt"
      },
      "outputs": [],
      "source": [
        "dim_x = 784\n",
        "dim_h = 100\n",
        "dim_out = 10\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(dim_x, dim_h),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(dim_h, dim_out),\n",
        ")\n",
        "\n",
        "learning_rate = 1e-2\n",
        "epochs = 2\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Using GPUs in PyTorch is pretty straightforward\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using cuda\")\n",
        "    use_cuda = True\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "# we need to tell pytorch to move the model to gpu\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    print(epoch)\n",
        "    for i, (xb, yb) in enumerate(train_dl):\n",
        "\n",
        "        # We also need to transfer the data to the target device\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        xb = xb.view(xb.size(0), -1)\n",
        "\n",
        "        # Forward pass\n",
        "        y_pred = model(xb)\n",
        "        loss = loss_fn(y_pred, yb)\n",
        "\n",
        "        # Backward pass\n",
        "        model.zero_grad()  # Zero out the previous gradient computation\n",
        "        loss.backward()  # Compute the gradient\n",
        "        optimizer.step()  # Use the gradient information to make a step\n",
        "\n",
        "    test_loss, test_acc = get_test_stat(model, test_dl, device)\n",
        "    print(\"Test loss: {}  Test acc: {}\".format(test_loss, test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSTuUZlRgeUD"
      },
      "source": [
        "### Dynamic network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji_qMWsOgbbj"
      },
      "source": [
        "To showcase the power of PyTorch dynamic graphs, we will implement a very strange model: a fully-connected ReLU network that on each forward pass randomly chooses a number between 1 and 4 and has that many hidden layers, reusing the same weights multiple times to compute the innermost hidden layers.\n",
        "\n",
        "By Justin Johnson: https://github.com/jcjohnson/pytorch-examples/blob/master/nn/dynamic_net.py."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5dRsCN9gnvU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "class DynamicNet(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        \"\"\"\n",
        "        In the constructor we construct three nn.Linear instances that we will use\n",
        "        in the forward pass.\n",
        "        \"\"\"\n",
        "        super(DynamicNet, self).__init__()\n",
        "        self.input_linear = torch.nn.Linear(D_in, H)\n",
        "        self.middle_linear = torch.nn.Linear(H, H)\n",
        "        self.output_linear = torch.nn.Linear(H, D_out)\n",
        "\n",
        "    def forward(self, x, verbose=False):\n",
        "        \"\"\"\n",
        "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
        "        and reuse the middle_linear Module that many times to compute hidden layer\n",
        "        representations.\n",
        "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
        "        Python control-flow operators like loops or conditional statements when\n",
        "        defining the forward pass of the model.\n",
        "        Here we also see that it is perfectly safe to reuse the same Module many\n",
        "        times when defining a computational graph. This is a big improvement from Lua\n",
        "        Torch, where each Module could be used only once.\n",
        "        \"\"\"\n",
        "        h_relu = self.input_linear(x).clamp(min=0)\n",
        "        n_layers = random.randint(0, 3)\n",
        "        if verbose:\n",
        "            print(\"The number of layers for this run is\", n_layers)\n",
        "            # print(h_relu)\n",
        "        for _ in range(n_layers):\n",
        "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
        "            if verbose:\n",
        "                pass\n",
        "                # print(h_relu)\n",
        "        y_pred = self.output_linear(h_relu)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 10, 1\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out).requires_grad_(False)\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = DynamicNet(D_in, H, D_out)\n",
        "\n",
        "# Construct our loss function and an Optimizer. Training this strange model with\n",
        "# vanilla stochastic gradient descent is tough, so we use momentum\n",
        "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
        "for t in range(50):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y)\n",
        "    if t % 10 == 0:\n",
        "        print(t, loss.data.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX-mRC_5iNvs"
      },
      "source": [
        "### CIFAR10 classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVftofphh-ng"
      },
      "source": [
        "We will finish with an example on [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html), highlighting the importance of applying transformations to your inputs.\n",
        "\n",
        "Example is lifted from: https://github.com/uoguelph-mlrg/Cutout/blob/master/train.py."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iAw2caKiW97"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 120)\n",
        "        self.fc3 = nn.Linear(120, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3p2SQViibIc"
      },
      "outputs": [],
      "source": [
        "def get_data(data_normalize=False, data_augment=False):\n",
        "    train_transform = transforms.Compose([])\n",
        "    test_transform = transforms.Compose([])\n",
        "\n",
        "    if data_augment:\n",
        "        train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n",
        "        train_transform.transforms.append(transforms.RandomHorizontalFlip())\n",
        "\n",
        "    train_transform.transforms.append(transforms.ToTensor())\n",
        "    test_transform.transforms.append(transforms.ToTensor())\n",
        "\n",
        "    if data_normalize:\n",
        "        normalize = transforms.Normalize(\n",
        "            mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
        "            std=[x / 255.0 for x in [63.0, 62.1, 66.7]],\n",
        "        )\n",
        "        train_transform.transforms.append(normalize)\n",
        "        test_transform.transforms.append(normalize)\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=\"data/\", train=True, transform=train_transform, download=True\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.CIFAR10(\n",
        "        root=\"data/\", train=False, transform=test_transform, download=True\n",
        "    )\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_dataset, batch_size=128, shuffle=True, num_workers=2\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        dataset=test_dataset, batch_size=128, shuffle=False, num_workers=2\n",
        "    )\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def test(net, loader):\n",
        "    net.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    for images, labels in loader:\n",
        "        with torch.no_grad():\n",
        "            pred = net(images)\n",
        "\n",
        "        pred = torch.max(pred.data, 1)[1]\n",
        "        total += labels.size(0)\n",
        "        correct += (pred == labels).sum().item()\n",
        "\n",
        "    val_acc = correct / total\n",
        "    net.train()\n",
        "    return val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GR395vyihIU"
      },
      "outputs": [],
      "source": [
        "def train_model(train_loader, test_loader, epochs=5):\n",
        "\n",
        "    net = Net()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(epoch)\n",
        "\n",
        "        xentropy_loss_avg = 0.0\n",
        "        correct = 0.0\n",
        "        total = 0.0\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "            net.zero_grad()\n",
        "            pred = net(images)\n",
        "            xentropy_loss = criterion(pred, labels)\n",
        "            xentropy_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            xentropy_loss_avg += xentropy_loss.item()\n",
        "\n",
        "            # Calculate running average of accuracy\n",
        "            pred = torch.max(pred.data, 1)[1]\n",
        "            total += labels.size(0)\n",
        "            correct += (pred == labels.data).sum().item()\n",
        "            accuracy = correct / total\n",
        "\n",
        "        test_acc = test(net, test_loader)\n",
        "        print(\"Test acc: \", test_acc)\n",
        "        train_accs.append(accuracy)\n",
        "        test_accs.append(test_acc)\n",
        "    return train_accs, test_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MADQb81UijK7"
      },
      "outputs": [],
      "source": [
        "train_loader, test_loader = get_data(data_augment=False, data_normalize=False)\n",
        "train_accs, test_accs = train_model(train_loader, test_loader, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnYCP-REikzs"
      },
      "outputs": [],
      "source": [
        "train_loader, test_loader = get_data(data_augment=False, data_normalize=True)\n",
        "normalize_train_accs, normalize_test_accs = train_model(train_loader, test_loader, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRwFghLzimSJ"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "epochs = 3\n",
        "ax.plot(range(epochs), train_accs, c=\"red\", label=\"without input normalization\")\n",
        "ax.plot(range(epochs), normalize_train_accs, c=\"blue\", label=\"with input normalization\")\n",
        "ax.legend()\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Accuracy\")\n",
        "ax.set_title(\"Train Accuracy\")\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}